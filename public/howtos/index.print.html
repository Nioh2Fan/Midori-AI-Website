<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.125.7">
    <meta name="generator" content="Relearn 5.25.0+tip">
    <meta name="robots" content="noindex, nofollow, noarchive, noimageindex">
    <meta name="description" content="Documentation for Midori-AI">
    <meta name="author" content="Luna Midori">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="LocalAI How-tos :: Midori-AI">
    <meta name="twitter:description" content="Documentation for Midori-AI">
    <meta property="og:title" content="LocalAI How-tos :: Midori-AI">
    <meta property="og:description" content="Documentation for Midori-AI">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:1313/howtos/index.html">
    <meta property="og:site_name" content="Midori-AI">
    <title>LocalAI How-tos :: Midori-AI</title>
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="/css/fontawesome-all.min.css?1724647731" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fontawesome-all.min.css?1724647731" rel="stylesheet"></noscript>
    <link href="/css/nucleus.css?1724647731" rel="stylesheet">
    <link href="/css/auto-complete.css?1724647731" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/auto-complete.css?1724647731" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar.min.css?1724647731" rel="stylesheet">
    <link href="/css/fonts.css?1724647731" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fonts.css?1724647731" rel="stylesheet"></noscript>
    <link href="/css/theme.css?1724647731" rel="stylesheet">
    <link href="/css/theme-auto.css?1724647731" rel="stylesheet" id="R-variant-style">
    <link href="/css/chroma-auto.css?1724647731" rel="stylesheet" id="R-variant-chroma-style">
    <link href="/css/variant.css?1724647731" rel="stylesheet">
    <link href="/css/print.css?1724647731" rel="stylesheet" media="print">
    <link href="/css/format-print.css?1724647731" rel="stylesheet">
    <link href="/css/ie.css?1724647731" rel="stylesheet">
    <script src="/js/url.js?1724647731"></script>
    <script src="/js/variant.js?1724647731"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      window.index_js_url="/index.search.js";
      var root_url="/";
      var baseUri=root_url.replace(/\/$/, '');
      window.relearn = window.relearn || {};
      window.relearn.baseUriFull='http:\/\/localhost:1313/';
      // variant stuff
      window.relearn.themeVariantModifier='';
      window.variants && variants.init( [ 'auto', 'relearn-bright', 'relearn-light', 'relearn-dark', 'learn', 'neon', 'blue', 'green', 'red' ] );
      // translations
      window.T_Copy_to_clipboard = `Copy to clipboard`;
      window.T_Copied_to_clipboard = `Copied to clipboard!`;
      window.T_Copy_link_to_clipboard = `Copy link to clipboard`;
      window.T_Link_copied_to_clipboard = `Copied link to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
    </script>
  </head>
  <body class="mobile-support print disableInlineCopyToClipboard" data-url="/howtos/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide"><button class="topbar-control" onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li
            itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><span itemprop="name">LocalAI How-tos</span><meta itemprop="position" content="1"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <article class="default">
            <header class="headline">
            </header>
<h1 id="localai-how-tos">LocalAI How-tos</h1>

<h2 id="how-tos">How-tos</h2>
<p><em>These are the <a href="https://localai.io/" target="_blank">LocalAI</a> How tos - <a href="https://localai.io/" target="_blank">Return to LocalAI</a></em></p>
<p>This section includes LocalAI end-to-end examples, tutorial and how-tos curated by the community and maintained by <a href="https://github.com/lunamidori5" target="_blank">lunamidori5</a>.
To add your own How Tos, Please open a PR on this github - <a href="https://github.com/lunamidori5/Midori-AI-Website/tree/master/content/howtos" target="_blank">https://github.com/lunamidori5/Midori-AI-Website/tree/master/content/howtos</a></p>
<ul>
<li><a href="/howtos/by_hand/easy-setup-docker/index.html">Setup LocalAI with Docker</a></li>
<li><a href="/howtos/by_hand/easy-model/index.html">Seting up a Model</a></li>
<li><a href="/howtos/by_hand/easy-request/index.html">Making Text / LLM requests to LocalAI</a></li>
<li><a href="/howtos/by_hand/easy-setup-sd/index.html">Making Photo / SD requests to LocalAI</a></li>
</ul>
<h2 id="programs-and-demos">Programs and Demos</h2>
<p>This section includes other programs and how to setup, install, and use of LocalAI.</p>
<ul>
<li><a href="/subsystem/manager/index.html">Midori AI Subsystem Manager</a> - <a href="https://github.com/lunamidori5" target="_blank">lunamidori5</a></li>
<li><a href="/howtos/setup-with-ha/index.html">HA-OS Info</a> - <a href="https://github.com/Anto79-ops" target="_blank">anto79_ops</a></li>
<li><a href="/howtos/homellmxlocalai/index.html">HA-OS x LocalAI</a> - <a href="https://github.com/maxi1134" target="_blank">Maxi1134</a></li>
<li><a href="/howtos/voice_assistance_guide/index.html">Voice Assistance</a> - <a href="https://github.com/maxi1134" target="_blank">Maxi1134</a></li>
</ul>
<h2 id="thank-you-to-our-collaborators-and-volunteers">Thank you to our collaborators and volunteers</h2>
<ul>
<li><a href="https://github.com/TwinFinz" target="_blank">TwinFinz</a>: Help with the models template files and reviewing some code</li>
<li><a href="https://github.com/dionysius" target="_blank">Crunchy</a>: PR helping with both installers and removing 7zip need</li>
<li><a href="https://github.com/maxi1134" target="_blank">Maxi1134</a>: Making our new HA-OS page for setting up LLM with HA</li>
<li><a href="/howtos/index.html"></a></li>
</ul>

            <footer class="footline">
            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of LocalAI How-tos</h1>
          <article class="default">
            <header class="headline">
            </header>
<h1 id="easy-model-setup">Easy Model Setup</h1>

<h2 id="------midori-ai-subsystem-manager------">&mdash;&ndash; Midori AI Subsystem Manager &mdash;&ndash;</h2>
<p>Use the model installer to install all of the base models like <code>Llava</code>, <code>tts</code>, <code>Stable Diffusion</code>, and more! <a href="/subsystem/manager/index.html">Click Here</a></p>
<h2 id="------by-hand-setup------">&mdash;&ndash; By Hand Setup &mdash;&ndash;</h2>
<p><em>(You do not have to run these steps if you have already done the auto manager)</em></p>
<p>Lets learn how to setup a model, for this <code>How To</code> we are going to use the <code>Dolphin Mistral 7B</code> model.</p>
<p>To download the model to your models folder, run this command in a commandline of your picking.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl -O https://tea-cup.midori-ai.xyz/download/7bmodelQ5.gguf</span></span></code></pre></div><p>Each model needs at least <code>4</code> files, with out these files, the model will run raw, what that means is you can not change settings of the model.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">File 1 - The model&#39;s GGUF file
</span></span><span class="line"><span class="cl">File 2 - The model&#39;s .yaml file
</span></span><span class="line"><span class="cl">File 3 - The Chat API .tmpl file
</span></span><span class="line"><span class="cl">File 4 - The Chat API helper .tmpl file</span></span></code></pre></div><p>So lets fix that! We are using <code>lunademo</code> name for this <code>How To</code> but you can name the files what ever you want! Lets make blank files to start with</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">touch lunademo-chat.tmpl
</span></span><span class="line"><span class="cl">touch lunademo-chat-block.tmpl
</span></span><span class="line"><span class="cl">touch lunademo.yaml</span></span></code></pre></div><p>Now lets edit the <code>&quot;lunademo-chat-block.tmpl&quot;</code>, This is the template that model &ldquo;Chat&rdquo; trained models use, but changed for LocalAI</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-txt" data-lang="txt"><span class="line"><span class="cl">&lt;|im_start|&gt;{{if eq .RoleName &#34;assistant&#34;}}assistant{{else if eq .RoleName &#34;system&#34;}}system{{else if eq .RoleName &#34;user&#34;}}user{{end}}
</span></span><span class="line"><span class="cl">{{if .Content}}{{.Content}}{{end}}
</span></span><span class="line"><span class="cl">&lt;|im_end|&gt;</span></span></code></pre></div><p>For the <code>&quot;lunademo-chat.tmpl&quot;</code>, Looking at the huggingface repo, this model uses the <code>&lt;|im_start|&gt;assistant</code> tag for when the AI replys, so lets make sure to add that to this file. Do not add the user as we will be doing that in our yaml file!</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-txt" data-lang="txt"><span class="line"><span class="cl">{{.Input}}
</span></span><span class="line"><span class="cl">&lt;|im_start|&gt;assistant</span></span></code></pre></div><p>For the <code>&quot;lunademo.yaml&quot;</code> file. Lets set it up for your computer or hardware. (If you want to see advanced yaml configs - <a href="https://localai.io/advanced/" target="_blank">Link</a>)</p>
<p>We are going to 1st setup the backend and context size.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">context_size</span><span class="p">:</span><span class="w"> </span><span class="m">2000</span></span></span></code></pre></div><p>What this does is tell <code>LocalAI</code> how to load the model. Then we are going to <strong>add</strong> our settings in after that. Lets add the models name and the models settings. The models <code>name:</code> is what you will put into your request when sending a <code>OpenAI</code> request to <code>LocalAI</code></p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lunademo</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">parameters</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l">7bmodelQ5.gguf</span></span></span></code></pre></div><p>Now that LocalAI knows what file to load with our request, lets add the stopwords and template files to our models yaml file now.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">stopwords</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;user|&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;assistant|&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;system|&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;&lt;|im_end|&gt;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;&lt;|im_start|&gt;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">chat</span><span class="p">:</span><span class="w"> </span><span class="l">lunademo-chat</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">chat_message</span><span class="p">:</span><span class="w"> </span><span class="l">lunademo-chat-block</span></span></span></code></pre></div><p>If you are running on <code>GPU</code> or want to tune the model, you can add settings like (higher the GPU Layers the more GPU used)</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">f16</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">gpu_layers</span><span class="p">:</span><span class="w"> </span><span class="m">4</span></span></span></code></pre></div><p>To fully tune the model to your like. But be warned, you <strong>must</strong> restart <code>LocalAI</code> after changing a yaml file</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker compose restart</span></span></code></pre></div><p>If you want to check your models yaml, here is a full copy!</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">context_size</span><span class="p">:</span><span class="w"> </span><span class="m">2000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c">##Put settings right here for tunning!! Before name but after Backend! (remove this comment before saving the file)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">lunademo</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">parameters</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l">7bmodelQ5.gguf</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">stopwords</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;user|&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;assistant|&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;system|&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;&lt;|im_end|&gt;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span>- <span class="s2">&#34;&lt;|im_start|&gt;&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">chat</span><span class="p">:</span><span class="w"> </span><span class="l">lunademo-chat</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">chat_message</span><span class="p">:</span><span class="w"> </span><span class="l">lunademo-chat-block</span></span></span></code></pre></div><p>Now that we got that setup, lets test it out but sending a <a href="/howtos/by_hand/easy-request/index.html">request</a> to Localai!</p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
            <header class="headline">
            </header>
<h1 id="easy-setup---docker">Easy Setup - Docker</h1>


<div class="box notices cstyle note">
  <div class="box-label"><i class="fa-fw fas fa-exclamation-circle"></i> Note</div>
  <div class="box-content">

<p>It is highly recommended to check out the <a href="/subsystem/manager/index.html">Midori AI Subsystem Manager</a> for setting up LocalAI. It does all of this for you!</p>
</div>
</div>

<div class="box notices cstyle Note">
  <div class="box-label"></div>
  <div class="box-content">

<ul>
<li>You will need about 10gb of RAM Free</li>
<li>You will need about 15gb of space free on C drive for <code>Docker compose</code></li>
</ul>
</div>
</div>
<p>We are going to run <code>LocalAI</code> with <code>docker compose</code> for this set up.</p>
<p>Lets setup our folders for <code>LocalAI</code> (run these to make the folders for you if you wish)</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-batch" data-lang="batch"><span class="line"><span class="cl"><span class="k">mkdir</span> <span class="s2">&#34;LocalAI&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">cd</span> LocalAI
</span></span><span class="line"><span class="cl"><span class="k">mkdir</span> <span class="s2">&#34;models&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">mkdir</span> <span class="s2">&#34;images&#34;</span></span></span></code></pre></div><p>At this point we want to set up our <code>.env</code> file, here is a copy for you to use if you wish, Make sure this is in the <code>LocalAI</code> folder.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1">## Set number of threads.</span>
</span></span><span class="line"><span class="cl"><span class="c1">## Note: prefer the number of physical cores. Overbooking the CPU degrades performance notably.</span>
</span></span><span class="line"><span class="cl"><span class="nv">THREADS</span><span class="o">=</span><span class="m">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Specify a different bind address (defaults to &#34;:8080&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ADDRESS=127.0.0.1:8080</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Define galleries.</span>
</span></span><span class="line"><span class="cl"><span class="c1">## models will to install will be visible in `/models/available`</span>
</span></span><span class="line"><span class="cl"><span class="nv">GALLERIES</span><span class="o">=[{</span><span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;model-gallery&#34;</span>, <span class="s2">&#34;url&#34;</span>:<span class="s2">&#34;github:go-skynet/model-gallery/index.yaml&#34;</span><span class="o">}</span>, <span class="o">{</span><span class="s2">&#34;url&#34;</span>: <span class="s2">&#34;github:go-skynet/model-gallery/huggingface.yaml&#34;</span>,<span class="s2">&#34;name&#34;</span>:<span class="s2">&#34;huggingface&#34;</span><span class="o">}]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Default path for models</span>
</span></span><span class="line"><span class="cl"><span class="nv">MODELS_PATH</span><span class="o">=</span>/models
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Enable debug mode</span>
</span></span><span class="line"><span class="cl"><span class="nv">DEBUG</span><span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Disables COMPEL (Lets Stable Diffuser work)</span>
</span></span><span class="line"><span class="cl"><span class="nv">COMPEL</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Enable/Disable single backend (useful if only one GPU is available)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># SINGLE_ACTIVE_BACKEND=true</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Specify a build type. Available: cublas, openblas, clblas.</span>
</span></span><span class="line"><span class="cl"><span class="nv">BUILD_TYPE</span><span class="o">=</span>cublas
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">REBUILD</span><span class="o">=</span><span class="nb">true</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Enable go tags, available: stablediffusion, tts</span>
</span></span><span class="line"><span class="cl"><span class="c1">## stablediffusion: image generation with stablediffusion</span>
</span></span><span class="line"><span class="cl"><span class="c1">## tts: enables text-to-speech with go-piper </span>
</span></span><span class="line"><span class="cl"><span class="c1">## (requires REBUILD=true)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1">#GO_TAGS=tts</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Path where to store generated images</span>
</span></span><span class="line"><span class="cl"><span class="c1"># IMAGE_PATH=/tmp</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Specify a default upload limit in MB (whisper)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># UPLOAD_LIMIT</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># HUGGINGFACEHUB_API_TOKEN=Token here</span></span></span></code></pre></div><p>Now that we have the <code>.env</code> set lets set up our <code>docker-compose.yaml</code> file.
It will use a container from <a href="https://quay.io/repository/go-skynet/local-ai?tab=tags" target="_blank">quay.io</a>.</p>

<div class="tab-panel" data-tab-group="5e3529540ae37b7b5734be45f7a68dc4">
  <div class="tab-nav">
    <div class="tab-nav-title">&#8203;</div>
    <button
      data-tab-item="vanilla--cpu-images"
      class="tab-nav-button tab-panel-style cstyle initial active" tabindex="-1"
      onclick="switchTab('5e3529540ae37b7b5734be45f7a68dc4','vanilla--cpu-images')"
    >
      <span class="tab-nav-text">Vanilla / CPU Images</span>
    </button>
    <button
      data-tab-item="gpu-images-cuda-11"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('5e3529540ae37b7b5734be45f7a68dc4','gpu-images-cuda-11')"
    >
      <span class="tab-nav-text">GPU Images CUDA 11</span>
    </button>
    <button
      data-tab-item="gpu-images-cuda-12"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('5e3529540ae37b7b5734be45f7a68dc4','gpu-images-cuda-12')"
    >
      <span class="tab-nav-text">GPU Images CUDA 12</span>
    </button>
  </div>
  <div class="tab-content-container">
    <div
      data-tab-item="vanilla--cpu-images"
      class="tab-content tab-panel-style cstyle initial active">
      <div class="tab-content-text">

<p>Recommened Midori AI - LocalAI Images</p>
<ul>
<li><code>lunamidori5/midori_ai_subsystem_localai_cpu:master</code></li>
</ul>
<p>For a full list of tags or images please <a href="https://hub.docker.com/r/lunamidori5/midori_ai_subsystem_localai_cpu/tags" target="_blank">check our docker repo</a></p>
<p>Base LocalAI Images</p>
<ul>
<li><code>master</code></li>
<li><code>latest</code></li>
</ul>
<p>Core Images - Smaller images without predownload python dependencies</p>
</div>
    </div>
    <div
      data-tab-item="gpu-images-cuda-11"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">

<p>Images with Nvidia accelleration support</p>
<blockquote>
<p>If you do not know which version of CUDA do you have available, you can check with <code>nvidia-smi</code> or <code>nvcc --version</code></p>
</blockquote>
<p>Recommened Midori AI - LocalAI Images (Only Nvidia works for now)</p>
<ul>
<li><code>lunamidori5/midori_ai_subsystem_localai_nvidia_gpu:master</code></li>
<li><code>lunamidori5/midori_ai_subsystem_localai_hipblas_gpu:master</code></li>
<li><code>lunamidori5/midori_ai_subsystem_localai_intelf16_gpu:master</code></li>
<li><code>lunamidori5/midori_ai_subsystem_localai_intelf32_gpu:master</code></li>
</ul>
<p>For a full list of tags or images please <a href="https://hub.docker.com/r/lunamidori5/midori_ai_subsystem_localai_gpu/tags" target="_blank">check our docker repo</a></p>
<p>Base LocalAI Images</p>
<ul>
<li><code>master-cublas-cuda11</code></li>
<li><code>master-cublas-cuda11-core</code></li>
<li><code>master-cublas-cuda11-ffmpeg</code></li>
<li><code>master-cublas-cuda11-ffmpeg-core</code></li>
</ul>
<p>Core Images - Smaller images without predownload python dependencies</p>
</div>
    </div>
    <div
      data-tab-item="gpu-images-cuda-12"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">

<p>Images with Nvidia accelleration support</p>
<blockquote>
<p>If you do not know which version of CUDA do you have available, you can check with <code>nvidia-smi</code> or <code>nvcc --version</code></p>
</blockquote>
<p>Recommened Midori AI - LocalAI Images (Only Nvidia works for now)</p>
<ul>
<li><code>lunamidori5/midori_ai_subsystem_localai_nvidia_gpu:master</code></li>
<li><code>lunamidori5/midori_ai_subsystem_localai_hipblas_gpu:master</code></li>
<li><code>lunamidori5/midori_ai_subsystem_localai_intelf16_gpu:master</code></li>
<li><code>lunamidori5/midori_ai_subsystem_localai_intelf32_gpu:master</code></li>
</ul>
<p>For a full list of tags or images please <a href="https://hub.docker.com/r/lunamidori5/midori_ai_subsystem_localai_gpu/tags" target="_blank">check our docker repo</a></p>
<p>Base LocalAI Images</p>
<ul>
<li><code>master-cublas-cuda12</code></li>
<li><code>master-cublas-cuda12-core</code></li>
<li><code>master-cublas-cuda12-ffmpeg</code></li>
<li><code>master-cublas-cuda12-ffmpeg-core</code></li>
</ul>
<p>Core Images - Smaller images without predownload python dependencies</p>
</div>
    </div>
  </div>
</div>

<div class="tab-panel" data-tab-group="cffbf5e84cdd6fc7a9750bb8ed06e41c">
  <div class="tab-nav">
    <div class="tab-nav-title">&#8203;</div>
    <button
      data-tab-item="cpu-only"
      class="tab-nav-button tab-panel-style cstyle initial active" tabindex="-1"
      onclick="switchTab('cffbf5e84cdd6fc7a9750bb8ed06e41c','cpu-only')"
    >
      <span class="tab-nav-text">CPU Only</span>
    </button>
    <button
      data-tab-item="gpu-and-cpu"
      class="tab-nav-button tab-panel-style cstyle initial"
      onclick="switchTab('cffbf5e84cdd6fc7a9750bb8ed06e41c','gpu-and-cpu')"
    >
      <span class="tab-nav-text">GPU and CPU</span>
    </button>
  </div>
  <div class="tab-content-container">
    <div
      data-tab-item="cpu-only"
      class="tab-content tab-panel-style cstyle initial active">
      <div class="tab-content-text">

<p>Also note this <code>docker-compose.yaml</code> file is for <code>CPU</code> only.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-docker" data-lang="docker"><span class="line"><span class="cl">version: <span class="s1">&#39;3.6&#39;</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>services:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  localai-midori-ai-backend:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    image: lunamidori5/midori_ai_subsystem_localai_cpu:master<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="c1">## use this for localai&#39;s base </span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="c1">## image: quay.io/go-skynet/local-ai:master</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    tty: <span class="nb">true</span> <span class="c1"># enable colorized logs</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    restart: always <span class="c1"># should this be on-failure ?</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    ports:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - 8080:8080<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    env_file:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - .env<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    volumes:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - ./models:/models<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - ./images/:/tmp/generated/images/<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    command: <span class="o">[</span><span class="s2">&#34;/usr/bin/local-ai&#34;</span> <span class="o">]</span></span></span></code></pre></div></div>
    </div>
    <div
      data-tab-item="gpu-and-cpu"
      class="tab-content tab-panel-style cstyle initial">
      <div class="tab-content-text">

<p>Also note this <code>docker-compose.yaml</code> file is for <code>CUDA</code> only.</p>
<p>Please change the image to what you need.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-docker" data-lang="docker"><span class="line"><span class="cl">version: <span class="s1">&#39;3.6&#39;</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>services:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>  localai-midori-ai-backend:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    deploy:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      resources:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>        reservations:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>          devices:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>            - driver: nvidia<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>              count: <span class="m">1</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>              capabilities: <span class="o">[</span>gpu<span class="o">]</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="c1">## use this for localai&#39;s base </span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    <span class="c1">## image: quay.io/go-skynet/local-ai:CHANGEMETOIMAGENEEDED</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    image: lunamidori5/midori_ai_subsystem_localai_nvidia_gpu:master<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    tty: <span class="nb">true</span> <span class="c1"># enable colorized logs</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    restart: always <span class="c1"># should this be on-failure ?</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    ports:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - 8080:8080<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    env_file:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - .env<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    volumes:<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - ./models:/models<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>      - ./images/:/tmp/generated/images/<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span>    command: <span class="o">[</span><span class="s2">&#34;/usr/bin/local-ai&#34;</span> <span class="o">]</span></span></span></code></pre></div></div>
    </div>
  </div>
</div>
<p>Make sure to save that in the root of the <code>LocalAI</code> folder. Then lets spin up the Docker run this in a <code>CMD</code> or <code>BASH</code></p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker compose up -d --pull always</span></span></code></pre></div><p>Now we are going to let that set up, once it is done, lets check to make sure our huggingface / localai galleries are working (wait until you see this screen to do this)</p>
<p>You should see:</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">┌───────────────────────────────────────────────────┐
</span></span><span class="line"><span class="cl">│                   Fiber v2.42.0                   │
</span></span><span class="line"><span class="cl">│               http://127.0.0.1:8080               │
</span></span><span class="line"><span class="cl">│       (bound on host 0.0.0.0 and port 8080)       │
</span></span><span class="line"><span class="cl">│                                                   │
</span></span><span class="line"><span class="cl">│ Handlers ............. 1  Processes ........... 1 │
</span></span><span class="line"><span class="cl">│ Prefork ....... Disabled  PID ................. 1 │
</span></span><span class="line"><span class="cl">└───────────────────────────────────────────────────┘</span></span></code></pre></div><p>Now that we got that setup, lets go setup a <a href="/howtos/by_hand/easy-model/index.html">model</a></p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
            <header class="headline">
            </header>
<h1 id="easy-setup---embeddings">Easy Setup - Embeddings</h1>

<p>To install an embedding model, run the following command</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl http://localhost:8080/models/apply -H <span class="s2">&#34;Content-Type: application/json&#34;</span> -d <span class="s1">&#39;{
</span></span></span><span class="line"><span class="cl"><span class="s1">     &#34;id&#34;: &#34;model-gallery@bert-embeddings&#34;
</span></span></span><span class="line"><span class="cl"><span class="s1">   }&#39;</span>  </span></span></code></pre></div><p>When you would like to request the model from CLI you can do</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl http://localhost:8080/v1/embeddings <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  -H <span class="s2">&#34;Content-Type: application/json&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  -d <span class="s1">&#39;{
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#34;input&#34;: &#34;The food was delicious and the waiter...&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#34;model&#34;: &#34;bert-embeddings&#34;
</span></span></span><span class="line"><span class="cl"><span class="s1">  }&#39;</span></span></span></code></pre></div><p>See <a href="https://platform.openai.com/docs/api-reference/embeddings/object" target="_blank">OpenAI Embedding</a> for more info!</p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
            <header class="headline">
            </header>
<h1 id="easy-setup---stable-diffusion">Easy Setup - Stable Diffusion</h1>

<h2 id="------midori-ai-subsystem-manager------">&mdash;&ndash; Midori AI Subsystem Manager &mdash;&ndash;</h2>
<p>Use the model installer to install all of the base models like <code>Llava</code>, <code>tts</code>, <code>Stable Diffusion</code>, and more! <a href="/subsystem/manager/index.html">Click Here</a></p>
<h2 id="------by-hand-setup------">&mdash;&ndash; By Hand Setup &mdash;&ndash;</h2>
<p><em>(You do not have to run these steps if you have already done the auto installer)</em></p>
<p>In your <code>models</code> folder make a file called <code>stablediffusion.yaml</code>, then edit that file with the following. (You can change <code>dreamlike-art/dreamlike-anime-1.0</code> with what ever model you would like.)</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">animagine</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">parameters</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="l">dreamlike-art/dreamlike-anime-1.0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">backend</span><span class="p">:</span><span class="w"> </span><span class="l">diffusers</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">cuda</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">f16</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">diffusers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">scheduler_type</span><span class="p">:</span><span class="w"> </span><span class="l">dpm_2_a</span></span></span></code></pre></div><p>If you are using docker, you will need to run in the localai folder with the <code>docker-compose.yaml</code> file in it</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker compose down</span></span></code></pre></div><p>Then in your <code>.env</code> file uncomment this line.</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="l">COMPEL=0</span></span></span></code></pre></div><p>After that we can reinstall the LocalAI docker VM by running in the localai folder with the <code>docker-compose.yaml</code> file in it</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker compose up -d</span></span></code></pre></div><p>Then to download and setup the model, Just send in a normal <code>OpenAI</code> request! LocalAI will do the rest!</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl http://localhost:8080/v1/images/generations -H <span class="s2">&#34;Content-Type: application/json&#34;</span> -d <span class="s1">&#39;{
</span></span></span><span class="line"><span class="cl"><span class="s1">  &#34;prompt&#34;: &#34;Two Boxes, 1blue, 1red&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">  &#34;model&#34;: &#34;animagine&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">  &#34;size&#34;: &#34;1024x1024&#34;
</span></span></span><span class="line"><span class="cl"><span class="s1">}&#39;</span></span></span></code></pre></div>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
            <header class="headline">
            </header>
<h1 id="easy-request---all">Easy Request - All</h1>

<h2 id="curl-request">Curl Request</h2>
<p>Curl Chat API -</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl http://localhost:8080/v1/chat/completions -H <span class="s2">&#34;Content-Type: application/json&#34;</span> -d <span class="s1">&#39;{
</span></span></span><span class="line"><span class="cl"><span class="s1">     &#34;model&#34;: &#34;lunademo&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;How are you?&#34;}],
</span></span></span><span class="line"><span class="cl"><span class="s1">     &#34;temperature&#34;: 0.9 
</span></span></span><span class="line"><span class="cl"><span class="s1">   }&#39;</span></span></span></code></pre></div><h2 id="openai-v1---recommended">Openai V1 - Recommended</h2>
<p>This is for Python, <code>OpenAI</code>=&gt;<code>V1</code></p>
<p>OpenAI Chat API Python -</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&#34;http://localhost:8080/v1&#34;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&#34;sk-xxx&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;You are LocalAI, a helpful, but really confused ai, you will only reply with confused emotes&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;Hello How are you today LocalAI&#34;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">=</span><span class="s2">&#34;lunademo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="p">)</span></span></span></code></pre></div><p>See <a href="https://platform.openai.com/docs/api-reference" target="_blank">OpenAI API</a> for more info!</p>
<h2 id="openai-v0---not-recommended">Openai V0 - Not Recommended</h2>
<p>This is for Python, <code>OpenAI</code>=<code>0.28.1</code></p>
<p>OpenAI Chat API Python -</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">openai</span>
</span></span><span class="line"><span class="cl"><span class="n">openai</span><span class="o">.</span><span class="n">api_base</span> <span class="o">=</span> <span class="s2">&#34;http://localhost:8080/v1&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&#34;sx-xxx&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">OPENAI_API_KEY</span> <span class="o">=</span> <span class="s2">&#34;sx-xxx&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">OPENAI_API_KEY</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">=</span><span class="s2">&#34;lunademo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;system&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;You are LocalAI, a helpful, but really confused ai, you will only reply with confused emotes&#34;</span><span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;How are you?&#34;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span></span></span></code></pre></div><p>OpenAI Completion API Python -</p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">openai</span>
</span></span><span class="line"><span class="cl"><span class="n">openai</span><span class="o">.</span><span class="n">api_base</span> <span class="o">=</span> <span class="s2">&#34;http://localhost:8080/v1&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&#34;sx-xxx&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">OPENAI_API_KEY</span> <span class="o">=</span> <span class="s2">&#34;sx-xxx&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">OPENAI_API_KEY</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Completion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span><span class="o">=</span><span class="s2">&#34;lunademo&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">prompt</span><span class="o">=</span><span class="s2">&#34;function downloadFile(string url, string outputPath) &#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span></span></span></code></pre></div>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
            <header class="headline">
            </header>
<h1 id="ha-os-homellm-x-localai">HA-OS (HomeLLM) x LocalAI</h1>

<hr>
<p>Home Assistant is an open-source home automation platform that allows users to control and monitor various smart devices in their homes. It supports a wide range of devices, including lights, thermostats, security systems, and more. The platform is designed  to be user-friendly and customizable, enabling users to create automations and routines to make their homes more convenient and efficient. Home Assistant can be accessed through a web interface or a mobile app, and it can be installed on a variety of hardware platforms, such as Raspberry Pi or a dedicated server.</p>
<p>Currently, Home Assistant supports conversation-based agents and services. As of writing this, OpenAIs API is supported as a conversation agent; however, access to your homes devices and entities is possible through custom components. Local based services, such as LocalAI, are also available as a drop-in replacement for OpenAI services.</p>
<hr>
<p>In this guide I will detail the steps I&rsquo;ve taken to get <a href="https://github.com/acon96/home-llm" target="_blank">Home-LLM</a> and <a href="https://github.com/mudler/LocalAI/" target="_blank">Local-AI</a> working together in conjunction with <a href="https://www.home-assistant.io/" target="_blank">Home-Assistant</a>!</p>
<p>This guide assumes that you already have <a href="https://github.com/mudler/LocalAI/" target="_blank">Local-AI</a> running (in or out of the subsystem).
If that is not done, you can <a href="/howtos/by_hand/easy-setup-docker/index.html">Follow this How To</a> or <a href="https://io.midori-ai.xyz/subsystem/manager/" target="_blank">Install Using Midori AI Subsystem</a>!</p>
<hr>
<ul>
<li>
<p>1: You will first need to <a href="https://github.com/acon96/home-llm/blob/develop/docs/Setup.md" target="_blank">follow this guide to install Home-LLM</a>into your <a href="https://www.home-assistant.io/" target="_blank">Home-Assistant</a> installation.</p>
<p>If you simply want to install the <a href="https://github.com/acon96/home-llm" target="_blank">Home-LLM</a> component through HACS,  you  can press on this button:</p>
<p><a href="https://my.home-assistant.io/redirect/hacs_repository/?category=Integration&repository=home-llm&owner=acon96" target="_blank">Open your Home Assistant instance and open a repository inside the Home Assistant Community Store.</a></p>
</li>
<li>
<p>2: Add <code>Home LLM Conversation</code> integration to HA.</p>
<ul>
<li>1: Access the <code>Settings</code> page.</li>
<li>2: Click on <code>Devices &amp; services</code>.</li>
<li>3: Click on <code>+ ADD INTEGRATION</code> on the lower-right part of the screen.</li>
<li>4: Type and then select <code>Local LLM Conversation</code>.</li>
<li>5: Select the <code>Generic OpenAI Compatible API</code>.</li>
<li>6: Enter the hostname or IP Address of your LocalAI host.</li>
<li>7: Enter the used port (Default is <code>8080</code> / <code>38080</code>).</li>
<li>8: Enter <code>mistral-7b-instruct-v0.3</code> as the <code>Model Name*</code>
<ul>
<li>Leave <code>API Key</code> empty</li>
<li>Do not check <code>Use HTTPS</code></li>
<li>leave <code>API Path*</code> as <code>/v1</code></li>
</ul>
</li>
<li>9: Press <code>Next</code></li>
<li>10: Select <code>Assist</code> under <code>Selected LLM API</code></li>
<li>11: Make sure the <code>Prompt Format*</code> is set to <code>Mistral</code></li>
<li>12: Make sure <code>Enable in context learning (ICL) examples</code> is checked.</li>
<li>13: Press <code>Sumbit</code></li>
<li>14: Press <code>Finish</code></li>
</ul>
</li>
</ul>
<p><a href="#R-image-448aa1eb04612993ab7eb982ca0c7b1f" class="lightbox-link"><img src="https://github.com/maxi1134/Home-Assistant-Config/blob/master/assets/home_llm_guide/home_llm_installation_video.gif?raw=true" alt="photo" class="figure-image noborder lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-448aa1eb04612993ab7eb982ca0c7b1f"><img src="https://github.com/maxi1134/Home-Assistant-Config/blob/master/assets/home_llm_guide/home_llm_installation_video.gif?raw=true" alt="photo" class="lightbox-image noborder lightbox noshadow" loading="lazy"></a></p>
<ul>
<li>
<p>3:  Configure the Voice assistant.</p>
<ul>
<li>1: Access the <code>Settings</code> page.</li>
<li>2: Click on <code>Voice assistants</code>.</li>
<li>3: Click on <code>+ ADD ASSISTANT</code>.</li>
<li>4: Name the Assistant <code>HomeLLM</code>.</li>
<li>5: Select <code>English</code> as the Language.</li>
<li>6: Set the <code>Conversation agent</code> to the newly created <code>LLM Model 'mistral-7b-instruct-v0.3' (remote)</code>.</li>
<li>7: Set your <code>Speech-to-text</code> <code>Wake word</code>, and <code>Text-to-speech</code> to the ones you use. Leave to <code>None</code> if you don&rsquo;t have any.</li>
<li>8: Click <code>Create</code></li>
</ul>
</li>
<li>
<p>4: Select the newly created voice assistant as the default one.</p>
<ul>
<li>While remaining on the <code>Voice assistants</code> page click on the newly create assistant, and press the start at the top-right corner.</li>
</ul>
</li>
</ul>
<p>There you go! Your Assistant should now be working with Local-AI through Home-LLM!</p>
<ul>
<li>Make sure that the entities you want to control are exposted to Assist within Home-Assistant!</li>
</ul>

<div class="box notices cstyle warning">
  <div class="box-label"><i class="fa-fw fas fa-exclamation-triangle"></i> Notice</div>
  <div class="box-content">

<p><strong>Important Note:</strong></p>
<p>Any devices you choose to expose to the model will be added to the context and may have their state changed by the model. Only expose devices that you are comfortable with the model modifying, even if the modification is not what you intended. The model may occasionally hallucinate and issue commands to the wrong device. Use at your own risk.</p>
</div>
</div>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
            <header class="headline">
            </header>
<h1 id="voice-assistant-ha-os">Voice Assistant HA-OS</h1>

<h3 id="in-this-guide-i-will-explain-how-ive-setup-my-local-voice-assistant-and-satellites">In this guide I will explain how I&rsquo;ve setup my Local voice assistant and satellites!</h3>
<p>A few softwares will be used in this guide.</p>
<p><a href="https://hacs.xyz/" target="_blank">HACS</a> for easy installation of the other tools on Home Assistant.<br>
<a href="https://localai.io/" target="_blank">LocalAI</a> for the backend of the LLM.<br>
<a href="https://github.com/acon96/home-llm" target="_blank">Home-LLM</a> to connect our LocalAI instance to Home-assistant.<br>
<a href="https://github.com/m50/ha-fallback-conversation" target="_blank">HA-Fallback-Conversation</a> to allow HA to use both the baked-in intent as well as the LLM as a fallback if no intent is found.<br>
<a href="https://heywillow.io/" target="_blank">Willow</a> for the ESP32 sattelites.</p>
<hr>
<h2 id="step-1-installing-localai">Step 1) Installing LocalAI</h2>
<p>We will start by installing <code>LocalAI</code> on our machine learning host.<br>
I recommend using a good machine with access to a GPU with at least 12 GB of Vram. As <code>Willow</code> itself can takes up to 6gb of Vram with another 4-5GB for our LLM model.   I recommend keeping those loaded in the machine at all time for speedy reaction times on our satellites.</p>
<p><strong>Here an example of the VRAM usage for  <code>Willow</code> and <code>LocalAI</code> with the <code>Llampa 8B</code> model:</strong></p>
<div class="wrap-code highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">+-----------------------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
</span></span><span class="line"><span class="cl">|-----------------------------------------+------------------------+----------------------+
</span></span><span class="line"><span class="cl">| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
</span></span><span class="line"><span class="cl">| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
</span></span><span class="line"><span class="cl">|                                         |                        |               MIG M. |
</span></span><span class="line"><span class="cl">|=========================================+========================+======================|
</span></span><span class="line"><span class="cl">|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |
</span></span><span class="line"><span class="cl">|  0%   39C    P8             16W /  370W |   10341MiB /  24576MiB |      0%      Default |
</span></span><span class="line"><span class="cl">|                                         |                        |                  N/A |
</span></span><span class="line"><span class="cl">+-----------------------------------------+------------------------+----------------------+
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| Processes:                                                                              |
</span></span><span class="line"><span class="cl">|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
</span></span><span class="line"><span class="cl">|        ID   ID                                                               Usage      |
</span></span><span class="line"><span class="cl">|=========================================================================================|
</span></span><span class="line"><span class="cl">|    0   N/A  N/A      2862      C   /opt/conda/bin/python                        3646MiB |
</span></span><span class="line"><span class="cl">|    0   N/A  N/A      2922      C   /usr/bin/python                              2108MiB |
</span></span><span class="line"><span class="cl">|    0   N/A  N/A   2724851      C   .../backend-assets/grpc/llama-cpp-avx2       4568MiB |
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------------------+</span></span></code></pre></div><p>I&rsquo;ve chosen the Docker-Compose method for my LocalAI installation, this allows for easy management and easier upgrades when new relases are available.<br>
This allows us to quickly create a container running LocalAI on our machine.</p>
<p>In order to do so, stop by the how to on how to setup a docker compose for LocalAI</p>
<p><a href="/howtos/by_hand/easy-setup-docker/index.html">Setup LocalAI with Docker Compose</a></p>
<p>Once that is done simply use <code>docker compose up -d</code> and your LocalAI instance should now be available at:
<code>http://(hostipadress):8080/</code></p>
<hr>
<h2 id="step-1a-downloading-the-llm-model">Step 1.a) Downloading the LLM model</h2>
<p>Once LocalAI if installed, you should be able to browse to the &ldquo;Models&rdquo; tab, that redirects to <code>http://{{host}}:8080/browse</code>. There we will search for the <code>mistral-7b-instruct-v0.3</code> model and install it.</p>
<p>Once that is done, make sure that the model is working by heading to the <code>Chat</code> tab and selecting the model <code>mistral-7b-instruct-v0.3</code> and initiating a chat.</p>
<p><a href="#R-image-8c2045e82a976764d59b269027fb1a3d" class="lightbox-link"><img src="https://github.com/maxi1134/Home-Assistant-Config/blob/master/assets/ai_guide/chat_example.png?raw=true" alt="alt text" class="figure-image noborder lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-8c2045e82a976764d59b269027fb1a3d"><img src="https://github.com/maxi1134/Home-Assistant-Config/blob/master/assets/ai_guide/chat_example.png?raw=true" alt="alt text" class="lightbox-image noborder lightbox noshadow" loading="lazy"></a></p>
<hr>
<h2 id="step-2-installing-home-llm">Step 2) Installing Home-LLM</h2>
<ul>
<li>
<p>1: You will first need to install the Home-LLM integration to Home-Assistant<br>
Thankfuly, there is a neat link to do that easely on <a href="https://github.com/acon96/home-llm" target="_blank">their repo</a>!</p>
<p><a href="https://my.home-assistant.io/redirect/hacs_repository/?category=Integration&repository=home-llm&owner=acon96" target="_blank">Open your Home Assistant instance and open a repository inside the Home Assistant Community Store.</a></p>
</li>
<li>
<p>2: Restart <code>Home Assistant</code></p>
</li>
<li>
<p>3: You will then need to add the  <code>Home LLM Conversation</code> integration to Home-Assistant in order to connect LocalAI to it.</p>
<ul>
<li>1: Access the <code>Settings</code> page.</li>
<li>2: Click on <code>Devices &amp; services</code>.</li>
<li>3: Click on <code>+ ADD INTEGRATION</code> on the lower-right part of the screen.</li>
<li>4: Type and then select <code>Local LLM Conversation</code>.</li>
<li>5: Select the <code>Generic OpenAI Compatible API</code>.</li>
<li>6: Enter the hostname or IP Address of your LocalAI host.</li>
<li>7: Enter the used port (Default is <code>8080</code>).</li>
<li>8: Enter <code>mistral-7b-instruct-v0.3</code> as the <code>Model Name*</code>
<ul>
<li>Leave <code>API Key</code> empty</li>
<li>Do not check <code>Use HTTPS</code></li>
<li>leave <code>API Path*</code> as <code>/v1</code></li>
</ul>
</li>
<li>9: Press <code>Next</code></li>
<li>10: Select <code>Assist</code> under <code>Selected LLM API</code></li>
<li>11: Make sure the <code>Prompt Format*</code> is set to <code>Mistral</code></li>
<li>12: Make sure <code>Enable in context learning (ICL) examples</code> is checked.</li>
<li>13: Press <code>Sumbit</code></li>
<li>14: Press <code>Finish</code></li>
</ul>
</li>
</ul>
<p><a href="#R-image-7c62205b1d624bca582c76588eb64627" class="lightbox-link"><img src="https://github.com/maxi1134/Home-Assistant-Config/blob/master/assets/home_llm_guide/home_llm_installation_video.gif?raw=true" alt="photo" class="figure-image noborder lightbox noshadow" style="height: auto; width: auto;" loading="lazy"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-7c62205b1d624bca582c76588eb64627"><img src="https://github.com/maxi1134/Home-Assistant-Config/blob/master/assets/home_llm_guide/home_llm_installation_video.gif?raw=true" alt="photo" class="lightbox-image noborder lightbox noshadow" loading="lazy"></a></p>
<hr>
<h2 id="step-3-installing-ha-fallback-conversationhttpsgithubcomm50ha-fallback-conversation">Step 3) Installing <a href="https://github.com/m50/ha-fallback-conversation" target="_blank">HA-Fallback-Conversation</a></h2>
<ul>
<li>
<p>1:  Integrate Fallback Conversation to Home-Assistant</p>
<ul>
<li>1: Access the <code>HACS</code> page.</li>
<li>2: Search for <code>Fallback</code></li>
<li>3: Click on <code>fallback_conversation</code>.</li>
<li>4: Click on <code>Download</code> and install the integration</li>
<li>5: Restart <code>Home Assistant</code> for the integration to be detected.</li>
<li>6: Access the <code>Settings</code> page.</li>
<li>7: Click on <code>Devices &amp; services</code>.</li>
<li>8: Click on <code>+ ADD INTEGRATION</code> on the lower-right part of the screen.</li>
<li>8: Search for <code>Fallback</code></li>
<li>9: Click on <code>Fallback Conversation Agent</code>.</li>
<li>10 Set the debug level at <code>Some Debug</code> for now.</li>
<li>11: Click <code>Sumbit</code></li>
<li></li>
</ul>
</li>
<li>
<p>2: Configure the Voice assistant within Home-assistant to use the newly added model through the <code>Fallback Conversation Agent</code>.</p>
<ul>
<li>1: Access the <code>Settings</code> page.</li>
<li>2: Click on <code>Devices &amp; services</code>.</li>
<li>3: Click on <code>Fallback Conversation Agent</code>.</li>
<li>4: Click on <code>CONFIGURE</code>.</li>
<li>5: Select <code>Home assistnat</code> as the <code>Primary Conversation Agent</code>.</li>
<li>6: Select <code>LLM MODEL 'mistral-7b-instruct-v0.3'(remote)</code> as the <code>Falback conversation Agent</code>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="step-4-selecting-the-right-agent-in-the-voice-assistant-settings">Step 4) Selecting the right agent in the Voice assistant settings.</h2>
<ul>
<li>1:  Integrate Fallback Conversation to Home-Assistant</li>
<li>1: Access the <code>Settings</code> page.</li>
<li>2: Click on <code>Voice assistants</code> page.</li>
<li>3: Click on <code>Add Assistant</code>.</li>
<li>4: Set the fields as wanted except for <code>Conversation Agent</code>.</li>
<li>5: Select <code>Fallback Conversation Agent</code> as the <code>Conversation agent</code>.</li>
</ul>
<hr>
<h2 id="step-5-setting-up-willow-voice-assistant-satellites">Step 5) Setting up Willow Voice assistant satellites.</h2>
<p>Since willow is a more complex Software, I will simply leave <a href="https://heywillow.io/quick-start-guide/" target="_blank">Their guide here</a>.
I do recommend deploying your own Willow Inference Server in order to remain completely local!</p>
<p>Once the Willow sattelites are connencted to <code>Home Assistant</code>, they should automatically use your default Voice Assistant.
Be sure to set the one using the fallback system as your favorite/default one!</p>

            <footer class="footline">
            </footer>
          </article>

          </section>
        </div>
      </main>
    </div>
    <script src="/js/clipboard.min.js?1724647731" defer></script>
    <script src="/js/perfect-scrollbar.min.js?1724647731" defer></script>
    <script src="/js/theme.js?1724647731" defer></script>
  </body>
</html>
