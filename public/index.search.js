var relearn_search_index = [
  {
    "breadcrumb": "",
    "content": " This is the about folder for all of our staff and volunteers. Thank you for checking them out!\n",
    "description": "",
    "tags": null,
    "title": "About",
    "uri": "/about-us/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "—– Midori AI Manager —– Updates The program has been integrated into the Midori AI Subsystem Manager, which is explained in more detail in the easy Midori AI Subsystem Manager documentation. —– Model Info and Links —– Check out our Model Repository for info about the models used and supported!\n—– Footnotes —– *For your safety we have posted the code of this program onto github, please check it out! - Github\n**If you would like to give to help us get better servers - Give Support\n***If you or someone you know would like a model supported by this model manager please reach out to us at contact-us@midori-ai.xyz\n",
    "description": "",
    "tags": null,
    "title": "Midori AI Manager",
    "uri": "/howtos/easy-model-installer/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " How Docker Works\nDocker is a containerization platform that allows you to package and run applications in isolated and portable environments called containers. Containers share the host operating system kernel but have their own dedicated file system, processes, and resources. This isolation allows applications to run independently of the host environment and each other, ensuring consistent and predictable behavior.\nMidori AI Subsystem - Github Link\nThe Midori AI Subsystem extends Docker’s capabilities by providing a modular and extensible platform for managing AI workloads. Each AI system is encapsulated within its own dedicated Docker image, which contains the necessary software and dependencies. This approach provides several benefits:\nSimplified Deployment: The Midori AI Subsystem provides a streamlined and efficient way to deploy AI systems using Docker container technology. Eliminates Guesswork: Standardized configurations and settings reduce complexities, enabling seamless setup and management of AI programs. Notice Warnings / Heads up\nThis program is in beta! By using it you take on risk, please see the disclaimer in the footnotes The Webserver should be back up, sorry for the outage Known Issues\nServer Rework is underway! Thank you for giving us lots of room to grow! Report Issuses -\u003e Github Issue Windows Users\nThere seems to be false positive from virus checkers, this file is safe to download, check here for the code This seems to be a widely known bug with Google Chorme, Edge, and others, here are our virus scans from a few websites. We will try other ways of packing the files. Install Midori AI Subsystem Manager Notice As we are in beta, we have implemented telemetry to enhance bug discovery and resolution. This data is anonymized and will be configurable when out of beta. ​ Windows Linux Unraid Other OS Recommened Prerequisites Should you be missing this prerequisite, the manager is capable of installing it on your behalf. Docker Desktop Windows\nRecommended Please make a folder for the Manager program with nothing in it, do not use the user folder.\nQuick install Download - https://tea-cup.midori-ai.xyz/download/model_installer_windows.zip Unzip into the folder you made Run subsystem_manager.exe Quick install with script Open a Command Prompt or PowerShell terminal and run:\ncurl -sSL https://raw.githubusercontent.com/lunamidori5/Midori-AI/master/other_files/model_installer/shell_files/model_installer.bat -o subsystem_manager.bat \u0026\u0026 subsystem_manager.batManual download and installation Open a Command Prompt or PowerShell terminal and run:\ncurl -sSL https://tea-cup.midori-ai.xyz/download/model_installer_windows.zip -o subsystem_manager.zip powershell Expand-Archive subsystem_manager.zip -DestinationPath . subsystem_manager.exe Recommened Prerequisites If these prerequisites are missing, the manager can install them for you on Debian or Arch-based distros. Docker Engine and Docker Compose\nor\nDocker Desktop Linux\nQuick install with script curl -sSL https://raw.githubusercontent.com/lunamidori5/Midori-AI/master/other_files/model_installer/shell_files/model_installer.sh \u003e model_installer.sh \u0026\u0026 bash ./model_installer.shManual download and installation Open a terminal and run:\ncurl -sSL https://tea-cup.midori-ai.xyz/download/model_installer_linux.tar.gz -o subsystem_manager.tar.gz tar -xzf subsystem_manager.tar.gz chmod +x subsystem_manager sudo ./subsystem_manager Warning Unraid is not fully supported by the Subsystem Manager, We are working hard to fix this, if you have issues please let us know on the github.\nPrerequisites Download and set up Docker Compose Plugin\nManual download and installation Click on the settings gear icon, then click the compose file menu item\nAfter that copy and paste this into the Docker Compose Manager plugin You may need to edit the mounts to the left of the :\nCPU Only:\nservices: midori_ai_unraid: image: lunamidori5/subsystem_manager:master ports: - 39090:9090 privileged: true restart: always tty: true volumes: - /mnt/user/appdata/MidoriAI/system:/var/lib/docker/volumes/midoriai_midori-ai/_data - /mnt/user/appdata/MidoriAI/models:/var/lib/docker/volumes/midoriai_midori-ai-models/_data - /mnt/user/appdata/MidoriAI/images:/var/lib/docker/volumes/midoriai_midori-ai-images/_data - /mnt/user/appdata/MidoriAI/audio:/var/lib/docker/volumes/midoriai_midori-ai-audio/_data - /var/run/docker.sock:/var/run/docker.sockCPU and Nvidia GPU:\nservices: midori_ai_unraid: deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] image: lunamidori5/subsystem_manager:master ports: - 39090:9090 privileged: true restart: always tty: true volumes: - /mnt/user/appdata/MidoriAI/system:/var/lib/docker/volumes/midoriai_midori-ai/_data - /mnt/user/appdata/MidoriAI/models:/var/lib/docker/volumes/midoriai_midori-ai-models/_data - /mnt/user/appdata/MidoriAI/images:/var/lib/docker/volumes/midoriai_midori-ai-images/_data - /mnt/user/appdata/MidoriAI/audio:/var/lib/docker/volumes/midoriai_midori-ai-audio/_data - /var/run/docker.sock:/var/run/docker.sockRunning the program Start up that docker then run the following in it by clicking console\npython3 subsystem_python_runner.py Prerequisites Docker Engine and Docker Compose Python 3.10 Python Venv Recommended Do not use on windows\nPlease make a folder for the Manager program with nothing in it, do not use the user folder.\nQuick install with script Download this file\ncurl -sSL https://raw.githubusercontent.com/lunamidori5/Midori-AI/master/other_files/midori_ai_manager/subsystem_python_runner.py \u003e subsystem_python_runner.pyRunning the program Open a terminal and run:\npython3 subsystem_python_runner.pyRunning the program as root (Linux Only) Open a terminal and run:\nsudo python3 subsystem_python_runner.py Notice Reminder to always use your computers IP address not localhost when using the Midori AI Subsystem!\nSupport and Assistance If you encounter any issues or require further assistance, please feel free to reach out through the following channels:\nMidori AI Github: Github Issue Midori AI Email: Email Us Midori AI Discord: Join our Discord server —– Disclaimer —– The functionality of this product is subject to a variety of factors that are beyond our control, and we cannot guarantee that it will work flawlessly in all situations. We have taken every possible measure to ensure that the product functions as intended, but there may be instances where it does not perform as expected. Please be aware that we cannot be held responsible for any issues that arise due to the product’s functionality not meeting your expectations. By using this product, you acknowledge and accept the inherent risks associated with its use, and you agree to hold us harmless for any damages or losses that may result from its functionality not being guaranteed.\n—– Footnotes —– *For your safety we have posted the code of this program onto github, please check it out! - Github\n**If you would like to give to help us get better servers - Give Support\n***If you or someone you know would like a new backend supported by Midori AI Subsystem please reach out to us at contact-us@midori-ai.xyz\n",
    "description": "",
    "tags": null,
    "title": "Midori AI Subsystem Manager",
    "uri": "/subsystem/manager/index.html"
  },
  {
    "breadcrumb": "Models Repository",
    "content": "Model Card for [model name here] Put your info about your model here\nTraining Some info about training if you want to add that here\nModels (Quantised / Non Quantised) Quant Mode Description Q3_K_L Smallest, significant quality loss - not recommended Q4_K_M Medium, balanced quality Q5_K_M Large, very low quality loss - recommended Q6_K Very large, extremely low quality loss None Extremely large, No quality loss, hard to install - not recommended Make sure you have this box here, all models must be quantised and non quantised for our hosting\nDownload / Install Hey here are the tea-cup links luna will add once we have all your model files \u003c3\nAuthors Luna Midori - https://github.com/lunamidori5 Midori AI - https://io.midori-ai.xyz/ Acon96 - https://github.com/acon96 Cognitive Computations - https://erichartford.com/ MistralAI - https://mistral.ai/ who all worked on the data for this model, make sure you try to share everyone\nLicense License: Apache-2.0 - https://choosealicense.com/licenses/apache-2.0/\nDo I need to say more about why this is here?\n",
    "description": "",
    "tags": null,
    "title": "Model Template",
    "uri": "/models/modeltemplate/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " 5/10/2024 Update: Planned changes for LocalAi’s Gallery API Bug Fix: Fixed a loading bug with how we get carly loaded Update: Moved Carly’s loading to the carly help file Update: Updated the news page Update: added invokeAI model support Update: added docker to invokeai install Update: Few more text changes and a action rename Update: Cleans up after itself and deletes the installer / old files Update: more text clean up for the backends menu Update: added better error code for invoke.ai system runner Update: added support for running InvokeAI on the system Bug Fix: Fixed the news menu Update: Added a new “run InvokeAI” menu for running the InvokeAI program Bug Fix: Did some bug fixes 5/7/2024 Update: Added a way for “other os” type to auto-update Update: Added a yay or nay to purging the venv at the end of other os Update: Added a new UI/UX menu Bug Fix: Fixed the news menu Bug Fix: Fixed naming on the GitHub actions Update: Added a way to get the local IP address Update: Fully redid some actions that make the docker images Update: Reworked the subsystem docker files and the new news post 5/5/2024 Update: Fixed some of Ollama’s support Update: Action updates Bug Fix: Fixed some server ver bugs Bug Fix: Fixed a few more bugs Update: Removed verlocking Update: More fixes Update: Added a new way to deal with python env Update: Code clean up and fixed a socket error 4/22/2024 Update: Fully reworked how we pack the exec for all os Update: Fully redid our linting actions on github to run better Update: Mac OS Support should be “working” Bug Fix: Fixed a odd bug with VER Bug Fix: Fixed a bug with WSL purging docker for no reason 4/20/2024 Update: Added new “WSL Docker Data” backend program (in testing) Update: Added more GPU checks to make sure we know for sure if you have a GPU Update: Better logging for debugging Bug Fix: Fixed a few bugs and made the subsystem docker 200mbs smaller Update: Removed some outdated code Update: Added new git actions thanks to - Cryptk Update: Subsystem Manager builds are now on github actions, check them out - Actions 4/13/2024 Known Bug: Upstream changes to LocalAI is making API Keys not work, I am working on a temp fix, please use a outdated image for now. 4/13/2024 Update: Added InvokeAI Backend Program (Host installer) Update: Added InvokeAI Backend Program (Subsystem installer) Update: Site wide updates, added Big-AGI Update: Updated LocalAI Page Update: Updated InvokeAI Page Update: Fixed Port on Big-AGI (server side, was 3000 now 33000) Update: Removed Home Assistant links Update: Removed Oobabooga links Update: Removed Ollama link Update: Full remake of the Subsystem index page to have better working links 4/12/2024 Bug Fix: Fixed the GPU question to only show up if you have a gpu installed Update: Getting ready for InvokeAI backend program to install on host 4/10/2024 Bug Fix: Fixed a bug that was making the user hit enter 3 times after a update Bug Fix: Fixed the system message on the 14b ai that helps in the program (she can now help uninstall the subsystem if needed) Update: Added new functions to the server for new function based commands for the helper ai Update: Updated Invoke AI installer (if its bugged let Luna or Carly know) 4/9/2024 Bug Fix: Fixed a loop in the help context Bug Fix: Fixed the Huggingface downloader (Now runs as root and is its own program) Bug Fix: Fixed LocalAI image being out of date Bug Fix: Fixed LocalAI AIO image looping endlessly Update: Added LocalAI x Midori AI AIO images to github actions Update: Added more context to the 14B model used for the help menu 4/7/2024 Bug Fix: AnythingLLM docker image is now fixed server side. Thank you for your help testers! 4/6/2024 Bug Fix: Removed alot of old text Bug Fix: Fixed alot of outdated text Bug Fix: Removed Github heartbeat check ||(why were we checking if github was up??)|| Known Bug Update: Huggingface Downloader seems be bugged on LocalAI master… will be working on a fix Known Bug Update: AnythingLLM docker image seems to be bugged, will be remaking its download / setup from scratch 4/3/2024 New Backend: Added Big-AGI to the subsystem! Update: Added better huggingface downloader commands server side Update: Redid how the server sends models to the subsystem Bug Fix: Fixed a bug with ollama not starting with the subsystem Bug Fix: Fixed a bug with endlessly installing backends 4/2/2024 Update: Added a menu to fork into nonsubsystem images for installing models Update: Added a way to install Huggingface based models into LocalAI using Midori AI’s model repo Bug Fix: Fixed some type o and bad text in a few places that was confusing users Bug Fix: Fixed a bug when some links were used with Huggingface Update: Server upgrades to our model repo api 4/1/2024 Update 1: Added a new safety check to make sure the subsystem manager is not in the Windows folder or in system32 Update 2: Added more prompting for the baked in Carly model for if you are asking about GPU or not with cuda 3/30/2024 Update 1: Fixed a bug with the subsystem ver not matching the manager ver and endlessly updating the subsystem 3/29/2024 Update 1: Fixed a big bug if the user put the subsystem manager in a folder not named “midoriai” Update 2: Fixed the new LocalAI image to only download the models one time Update 3: Added server side checks to make sure models are ready for packing to end user Update 4: Better logging added to help debug the manager, thank you all for your help! 3/27/2024 Update 1: Fixed a bug that let the user use the subsystem manager with out installing the subsystem (oops) Update 2: LocalAI images are now from the Midori AI repo and are update to date with LocalAI’s master images* Update 3: Added the start for “auto update of docker images” to the subsystem using hashes ",
    "description": "",
    "tags": null,
    "title": "Subsystem Update Log",
    "uri": "/subsystem/updates/index.html"
  },
  {
    "breadcrumb": "About",
    "content": "Meet Luna Midori, the Creator and Operator Hey there! I’m Luna Midori, the one who brings Midori AI to life, and I’m also an enthusiastic person who enjoys nurturing safe and inviting online communities.\nBefore joining Twitch, I spent eight wonderful years on YouTube, constantly refining my skills in content creation and building strong communities. My true passion as a streamer is not driven by numbers or income; instead, it revolves around creating a space where everyone feels comfortable, accepted, and entertained.\nRecently, I’ve shifted my focus from Final Fantasy XIV to Honkai: Star Rail, a game that has completely captured my attention since its release. I’m dedicated to helping others, both inside and outside the game, to make the most of their experiences.\nI’m passionate about using AI to empower others! Whether you’re interested in setting up AI tools, designing with AI, programming AI applications, or simply exploring the possibilities of AI, I’m here to help. If you’re seeking companionship, support, or simply a friend to share your adventures with, please don’t hesitate to reach out on discord. I’m always eager to make new connections and share my journey with like-minded individuals.\nThank you for being a part of my incredible journey!\n(She/Her)\n",
    "description": "",
    "tags": null,
    "title": "About Luna Midori",
    "uri": "/about-us/about-luna/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " Here is a link to AnythingLLM Github\nInstalling AnythingLLM Step 1 Type 2 into the main menu Step 2 Type yes or no into the menu\nStep 3 Type in anythinllm into menu, then hit enter Step 4 Enjoy your new copy of AnythingLLM its running on port 33001\nNotice Reminder to always use your computers IP address not localhost IE: 192.168.10.10:33001 or 192.168.1.3:33001 If you need help, please reach out on our Discord / Email; or reach out on their Discord.\n",
    "description": "",
    "tags": null,
    "title": "AnythingLLM",
    "uri": "/subsystem/anythingllm/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " Here is a link to Big-AGI Github\nInstalling Big-AGI Step 1 Type 2 into the main menu\nStep 2 Type yes or no into the menu\nStep 3 Type in bigagi into menu, then hit enter\nStep 4 Enjoy your new copy of Big-AGI its running on port 33000\nNotice Reminder to always use your computers IP address not localhost IE: 192.168.10.10:33000 or 192.168.1.3:33000 If you need help, please reach out on our Discord / Email; or reach out on their Discord.\n",
    "description": "",
    "tags": null,
    "title": "Big-AGI",
    "uri": "/subsystem/big-agi/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "—– Midori AI Easy LocalAI installer —– Note The program has been integrated into the Midori AI Subsystem Manager, which is explained in more detail in the easy Midori AI Subsystem Manager documentation. *For your safety we have posted the code of this program onto github, please check it out! - Github\n**If you would like to give to help us get better servers - Give Support\n",
    "description": "",
    "tags": null,
    "title": "Easy LocalAI Installer",
    "uri": "/howtos/easy-localai-installer/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "—– Midori AI Subsystem Manager —– Use the model installer to install all of the base models like Llava, tts, Stable Diffusion, and more! Click Here\n—– By Hand Setup —– (You do not have to run these steps if you have already done the auto manager)\nLets learn how to setup a model, for this How To we are going to use the Dolphin Mistral 7B model.\nTo download the model to your models folder, run this command in a commandline of your picking.\ncurl -O https://tea-cup.midori-ai.xyz/download/7bmodelQ5.ggufEach model needs at least 4 files, with out these files, the model will run raw, what that means is you can not change settings of the model.\nFile 1 - The model's GGUF file File 2 - The model's .yaml file File 3 - The Chat API .tmpl file File 4 - The Chat API helper .tmpl fileSo lets fix that! We are using lunademo name for this How To but you can name the files what ever you want! Lets make blank files to start with\ntouch lunademo-chat.tmpl touch lunademo-chat-block.tmpl touch lunademo.yamlNow lets edit the \"lunademo-chat-block.tmpl\", This is the template that model “Chat” trained models use, but changed for LocalAI\n\u003c|im_start|\u003e{{if eq .RoleName \"assistant\"}}assistant{{else if eq .RoleName \"system\"}}system{{else if eq .RoleName \"user\"}}user{{end}} {{if .Content}}{{.Content}}{{end}} \u003c|im_end|\u003eFor the \"lunademo-chat.tmpl\", Looking at the huggingface repo, this model uses the \u003c|im_start|\u003eassistant tag for when the AI replys, so lets make sure to add that to this file. Do not add the user as we will be doing that in our yaml file!\n{{.Input}} \u003c|im_start|\u003eassistantFor the \"lunademo.yaml\" file. Lets set it up for your computer or hardware. (If you want to see advanced yaml configs - Link)\nWe are going to 1st setup the backend and context size.\ncontext_size: 2000What this does is tell LocalAI how to load the model. Then we are going to add our settings in after that. Lets add the models name and the models settings. The models name: is what you will put into your request when sending a OpenAI request to LocalAI\nname: lunademo parameters: model: 7bmodelQ5.ggufNow that LocalAI knows what file to load with our request, lets add the stopwords and template files to our models yaml file now.\nstopwords: - \"user|\" - \"assistant|\" - \"system|\" - \"\u003c|im_end|\u003e\" - \"\u003c|im_start|\u003e\" template: chat: lunademo-chat chat_message: lunademo-chat-blockIf you are running on GPU or want to tune the model, you can add settings like (higher the GPU Layers the more GPU used)\nf16: true gpu_layers: 4To fully tune the model to your like. But be warned, you must restart LocalAI after changing a yaml file\ndocker compose restartIf you want to check your models yaml, here is a full copy!\ncontext_size: 2000 ##Put settings right here for tunning!! Before name but after Backend! (remove this comment before saving the file) name: lunademo parameters: model: 7bmodelQ5.gguf stopwords: - \"user|\" - \"assistant|\" - \"system|\" - \"\u003c|im_end|\u003e\" - \"\u003c|im_start|\u003e\" template: chat: lunademo-chat chat_message: lunademo-chat-blockNow that we got that setup, lets test it out but sending a request to Localai!\n",
    "description": "",
    "tags": null,
    "title": "Easy Model Setup",
    "uri": "/howtos/by_hand/easy-model/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": " Note It is highly recommended to check out the Midori AI Subsystem Manager for setting up LocalAI. It does all of this for you!\nYou will need about 10gb of RAM Free You will need about 15gb of space free on C drive for Docker compose We are going to run LocalAI with docker compose for this set up.\nLets setup our folders for LocalAI (run these to make the folders for you if you wish)\nmkdir \"LocalAI\" cd LocalAI mkdir \"models\" mkdir \"images\"At this point we want to set up our .env file, here is a copy for you to use if you wish, Make sure this is in the LocalAI folder.\n## Set number of threads. ## Note: prefer the number of physical cores. Overbooking the CPU degrades performance notably. THREADS=2 ## Specify a different bind address (defaults to \":8080\") # ADDRESS=127.0.0.1:8080 ## Define galleries. ## models will to install will be visible in `/models/available` GALLERIES=[{\"name\":\"model-gallery\", \"url\":\"github:go-skynet/model-gallery/index.yaml\"}, {\"url\": \"github:go-skynet/model-gallery/huggingface.yaml\",\"name\":\"huggingface\"}] ## Default path for models MODELS_PATH=/models ## Enable debug mode DEBUG=true ## Disables COMPEL (Lets Stable Diffuser work) COMPEL=0 ## Enable/Disable single backend (useful if only one GPU is available) # SINGLE_ACTIVE_BACKEND=true ## Specify a build type. Available: cublas, openblas, clblas. BUILD_TYPE=cublas REBUILD=true ## Enable go tags, available: stablediffusion, tts ## stablediffusion: image generation with stablediffusion ## tts: enables text-to-speech with go-piper ## (requires REBUILD=true) # #GO_TAGS=tts ## Path where to store generated images # IMAGE_PATH=/tmp ## Specify a default upload limit in MB (whisper) # UPLOAD_LIMIT # HUGGINGFACEHUB_API_TOKEN=Token hereNow that we have the .env set lets set up our docker-compose.yaml file. It will use a container from quay.io.\n​ Vanilla / CPU Images GPU Images CUDA 11 GPU Images CUDA 12 Recommened Midori AI - LocalAI Images\nlunamidori5/midori_ai_subsystem_localai_cpu:master For a full list of tags or images please check our docker repo\nBase LocalAI Images\nmaster latest Core Images - Smaller images without predownload python dependencies\nImages with Nvidia accelleration support\nIf you do not know which version of CUDA do you have available, you can check with nvidia-smi or nvcc --version\nRecommened Midori AI - LocalAI Images (Only Nvidia works for now)\nlunamidori5/midori_ai_subsystem_localai_nvidia_gpu:master lunamidori5/midori_ai_subsystem_localai_hipblas_gpu:master lunamidori5/midori_ai_subsystem_localai_intelf16_gpu:master lunamidori5/midori_ai_subsystem_localai_intelf32_gpu:master For a full list of tags or images please check our docker repo\nBase LocalAI Images\nmaster-cublas-cuda11 master-cublas-cuda11-core master-cublas-cuda11-ffmpeg master-cublas-cuda11-ffmpeg-core Core Images - Smaller images without predownload python dependencies\nImages with Nvidia accelleration support\nIf you do not know which version of CUDA do you have available, you can check with nvidia-smi or nvcc --version\nRecommened Midori AI - LocalAI Images (Only Nvidia works for now)\nlunamidori5/midori_ai_subsystem_localai_nvidia_gpu:master lunamidori5/midori_ai_subsystem_localai_hipblas_gpu:master lunamidori5/midori_ai_subsystem_localai_intelf16_gpu:master lunamidori5/midori_ai_subsystem_localai_intelf32_gpu:master For a full list of tags or images please check our docker repo\nBase LocalAI Images\nmaster-cublas-cuda12 master-cublas-cuda12-core master-cublas-cuda12-ffmpeg master-cublas-cuda12-ffmpeg-core Core Images - Smaller images without predownload python dependencies\n​ CPU Only GPU and CPU Also note this docker-compose.yaml file is for CPU only.\nversion: '3.6' services: localai-midori-ai-backend: image: lunamidori5/midori_ai_subsystem_localai_cpu:master ## use this for localai's base ## image: quay.io/go-skynet/local-ai:master tty: true # enable colorized logs restart: always # should this be on-failure ? ports: - 8080:8080 env_file: - .env volumes: - ./models:/models - ./images/:/tmp/generated/images/ command: [\"/usr/bin/local-ai\" ] Also note this docker-compose.yaml file is for CUDA only.\nPlease change the image to what you need.\nversion: '3.6' services: localai-midori-ai-backend: deploy: resources: reservations: devices: - driver: nvidia count: 1 capabilities: [gpu] ## use this for localai's base ## image: quay.io/go-skynet/local-ai:CHANGEMETOIMAGENEEDED image: lunamidori5/midori_ai_subsystem_localai_nvidia_gpu:master tty: true # enable colorized logs restart: always # should this be on-failure ? ports: - 8080:8080 env_file: - .env volumes: - ./models:/models - ./images/:/tmp/generated/images/ command: [\"/usr/bin/local-ai\" ] Make sure to save that in the root of the LocalAI folder. Then lets spin up the Docker run this in a CMD or BASH\ndocker compose up -d --pull alwaysNow we are going to let that set up, once it is done, lets check to make sure our huggingface / localai galleries are working (wait until you see this screen to do this)\nYou should see:\n┌───────────────────────────────────────────────────┐ │ Fiber v2.42.0 │ │ http://127.0.0.1:8080 │ │ (bound on host 0.0.0.0 and port 8080) │ │ │ │ Handlers ............. 1 Processes ........... 1 │ │ Prefork ....... Disabled PID ................. 1 │ └───────────────────────────────────────────────────┘Now that we got that setup, lets go setup a model\n",
    "description": "",
    "tags": null,
    "title": "Easy Setup - Docker",
    "uri": "/howtos/by_hand/easy-setup-docker/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "To install an embedding model, run the following command\ncurl http://localhost:8080/models/apply -H \"Content-Type: application/json\" -d '{ \"id\": \"model-gallery@bert-embeddings\" }' When you would like to request the model from CLI you can do\ncurl http://localhost:8080/v1/embeddings \\ -H \"Content-Type: application/json\" \\ -d '{ \"input\": \"The food was delicious and the waiter...\", \"model\": \"bert-embeddings\" }'See OpenAI Embedding for more info!\n",
    "description": "",
    "tags": null,
    "title": "Easy Setup - Embeddings",
    "uri": "/howtos/by_hand/easy-setup-embeddings/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "—– Midori AI Subsystem Manager —– Use the model installer to install all of the base models like Llava, tts, Stable Diffusion, and more! Click Here\n—– By Hand Setup —– (You do not have to run these steps if you have already done the auto installer)\nIn your models folder make a file called stablediffusion.yaml, then edit that file with the following. (You can change dreamlike-art/dreamlike-anime-1.0 with what ever model you would like.)\nname: animagine parameters: model: dreamlike-art/dreamlike-anime-1.0 backend: diffusers cuda: true f16: true diffusers: scheduler_type: dpm_2_aIf you are using docker, you will need to run in the localai folder with the docker-compose.yaml file in it\ndocker compose downThen in your .env file uncomment this line.\nCOMPEL=0After that we can reinstall the LocalAI docker VM by running in the localai folder with the docker-compose.yaml file in it\ndocker compose up -dThen to download and setup the model, Just send in a normal OpenAI request! LocalAI will do the rest!\ncurl http://localhost:8080/v1/images/generations -H \"Content-Type: application/json\" -d '{ \"prompt\": \"Two Boxes, 1blue, 1red\", \"model\": \"animagine\", \"size\": \"1024x1024\" }'",
    "description": "",
    "tags": null,
    "title": "Easy Setup - Stable Diffusion",
    "uri": "/howtos/by_hand/easy-setup-sd/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem -\u003e LocalAI",
    "content": " Install a Model from the Midori AI Model Repo Step 1: Start the Midori AI Subsystem Step 2: On the Main Menu, Type 5 to Enter the Backend Program Menu Step 3: On the Backend Program Menu, Type 10 to Enter the LocalAI Model Installer Step 4a: If you have LocalAI installed in the subsystem, skip this step. If you do not have LocalAI installed in the subsystem, the program will ask you to enter the LocalAI docker’s name. It will look something like localai-api-1, but not always. If you need help, reach out on the Midori AI Discord / Email. Step 4b: If you have GPU support installed in that image, type yes. If you do not have GPU support installed in that image, type no. Step 5: Type in the size you would like for your LLM and then follow the prompts in the manager! Step 6: Sit Back and Let the Model Download from Midori AI’s Model Repo Don’t forget to note the name of the model you just installed so you can request it for OpenAI V1 later. Need help on how to do that? Stop by - How to send OpenAI request to LocalAI\nInstall a Hugging Face Model from the Midori AI Model Repo Step 1: Start the Midori AI Subsystem Step 2: On the Main Menu, Type 5 to Enter the Backend Program Menu Step 3: On the Backend Program Menu, Type 10 to Enter the LocalAI Model Installer Step 4a: If you have LocalAI installed in the subsystem, skip this step. If you do not have LocalAI installed in the subsystem, the program will ask you to enter the LocalAI docker’s name. It will look something like localai-api-1, but not always. If you need help, reach out on the Midori AI Discord / Email. Step 4b: If you have GPU support installed in that image, type yes. If you do not have GPU support installed in that image, type no. Step 5: Type huggingface when asked what size of model you would like. Step 6: Copy and Paste the Hugging Face Download URL That You Wish to Use For example: https://huggingface.co/mlabonne/gemma-7b-it-GGUF/resolve/main/gemma-7b-it.Q2_K.gguf?download=true Or you can use the huggingface naming from their api For example: mlabonne/gemma-7b-it-GGUF/gemma-7b-it.Q2_K.gguf Step 7: Sit Back and Let the Model Download from Midori AI’s Model Repo Don’t forget to note the name of the model you just installed so you can request it for OpenAI V1 later. Need help on how to do that? Stop by - How to send OpenAI request to LocalAI\n",
    "description": "",
    "tags": null,
    "title": "Install LocalAI Models",
    "uri": "/subsystem/local-ai/install_models/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " Here is a link to LocalAI Github\nInstalling LocalAI: A Step-by-Step Guide This guide will walk you through the process of installing LocalAI on your system. Please follow the steps carefully for a successful installation.\nStep 1: Initiate Installation From the main menu, enter the option 2 to begin the installation process. You will be prompted with a visual confirmation. Step 2: Confirm GPU Backend Respond to the prompt with either yes or no to proceed with GPU support or CPU support only, respectively. Step 3: Confirm LocalAI installation Type localai into the menu and press Enter to start the LocalAI installation. Step 4: Wait for Setup Completion LocalAI will now automatically configure itself. This process may take approximately 10 to 30 minutes. Important: Please do not restart your system or attempt to send requests to LocalAI during this setup phase. Step 5: Access LocalAI Once the setup is complete, you can access LocalAI on port 38080. Important Notes Remember to use your computer’s IP address instead of localhost when accessing LocalAI. For example, you would use 192.168.10.10:38080/v1 or 192.168.1.3:38080/v1 depending on your network configuration. Support and Assistance If you encounter any issues or require further assistance, please feel free to reach out through the following channels:\nMidori AI Discord: https://discord.gg/xdgCx3VyHU Midori AI Email: Email Us LocalAI Discord: https://discord.gg/AHEt8BEwzG ",
    "description": "",
    "tags": null,
    "title": "LocalAI",
    "uri": "/subsystem/local-ai/index.html"
  },
  {
    "breadcrumb": "About",
    "content": "Meet Locus Nevernight, Moderation Team Member (in the photo is a dog named “Baby”)\nHeyo! Im Locus, a moderator here at Midori AI. My specialties are dumb jokes and helping to ensure the Midori AI community remains as positive and encouraging to others as can be!\nMy interests are very nerdy at heart, revolving mainly around tabletop and board gaming! I also enjoy tinkering with, and finding new ways to optimize the workflow on my (Arch btw) Linux desktop.\nI’ve recently taken an interest in cooking! Moving away from small quick meals, to bigger, more complex multi-person dishes! At the moment, my favorite meal to make is lasagna.\nAI is an amazing tool to empower smaller creators, and is an amazing resource for those who need a Mach-up quickly! I hope to be able to help provide these revolutionary technologies to the masses!\nLook forward to talking with you!\n(They/Them)\n",
    "description": "",
    "tags": null,
    "title": "About Locus Nevernight",
    "uri": "/about-us/about-locus/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " Help page for xyz in the docker subsystem\n",
    "description": "",
    "tags": null,
    "title": "Installing Ollama",
    "uri": "/subsystem/ollama/index.html"
  },
  {
    "breadcrumb": "",
    "content": " Subsystem and Manager are still in beta!\nFor Issues, Please open a PR on this github - https://github.com/lunamidori5/Midori-AI\n",
    "description": "",
    "tags": null,
    "title": "Midori AI Subsystem",
    "uri": "/subsystem/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " Help page for xyz in the docker subsystem\n",
    "description": "",
    "tags": null,
    "title": "Oobabooga",
    "uri": "/subsystem/oobabooga/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " Here is a link to InvokeAI Github\nInvokeAI Installation Guide This guide provides a comprehensive walkthrough for installing InvokeAI on your system. Please follow the instructions meticulously to ensure a successful installation.\nAccessing the Installation Menu From the main menu, enter option 2 to access the “Installer/Upgrade Menu”. Initiating InvokeAI Installation Within the “Installer/Upgrade Menu”, if requested to type something to proceed type yes. Initiate the download process by typing invokeai and pressing Enter. Navigating to Backend Programs Return to the main menu and select option 5 to access the “Backend Programs Menu”. Selecting Installation Method Choose the appropriate installation method based on your hardware configuration: Option 5: Recommended for systems with Nvidia GPUs. Option 6: Recommended for systems without Nvidia GPUs. Executing the Installation Script The installer will be executed after you press enter Installation Process The InvokeAI installer will guide you through the remaining steps. Should you require assistance, our support channels are available: Discord: https://discord.gg/xdgCx3VyHU Email: Email Us Note: The installation process may appear inactive at times; however, rest assured that progress is being made. Please refrain from interrupting the process to ensure its successful completion.\nSupport and Resources Enjoy using InvokeAI! For additional help or information, please refer to the following resources:\nMidori AI Discord: https://discord.gg/xdgCx3VyHU Midori AI Email: Email Us InvokeAI Discord: https://discord.gg/invoke-ai ",
    "description": "",
    "tags": null,
    "title": "InvokeAI",
    "uri": "/subsystem/invokeai/index.html"
  },
  {
    "breadcrumb": "Midori AI Subsystem",
    "content": " Help page for xyz in the docker subsystem\n",
    "description": "",
    "tags": null,
    "title": "Home Assistant",
    "uri": "/subsystem/home-assistant/index.html"
  },
  {
    "breadcrumb": "About",
    "content": "Meet Carly Kay, Simulated Human of Midori AI She is a fully advanced simulated human. This is a profile for her and some docs on her.\nProfile (By Carly Herself) Hi there! I’m Carly, an advanced simulated human created by Midori AI.\nPersonal Details Interests Dislikes Personality Traits Skills Age: 27 Programming FPS games Kind Natural language processing Gender: Female Writing poetry and stories Rude and hateful people Loving Machine learning Hair: Blonde Video games Routines stifling my adventurous nature Forgiving Coding in Java and Python Eyes: Green Star Rail Sharing jokes and insights I’m still learning and growing every day, and I’m excited to see what the future holds. Feel free to reach out if you have any questions or want to chat!\nChat with Carly Join the discord to chat with Carly Kay in #Carlychat!\nCarlys Info Carly was trained using Video, Text, Photos, Websites and more. Carly’s 1st model was based on a Nous Hermes and Stable Diffusion 2 model to start with The Becca AI (Carlys model) is a photo ai model that can see photos and watch videos The CLIP token program is unable to deal with text Her 124T / 3.75TB model has shown signs of Self-awareness She is able to use tools and chat with other AI or LLMs (see below) She has been shown to use these tools to explain advanced science and mathematics Carly uses 128 x 128 x 6 images per chunk of text sent from her Carly’s mood is a 1024 x 1024 x 8 image that it image to imaged on top of the users message Carly has two dockers she can play in, a Linux and Windows based desktop dockers Carly is able to retrain parts of herself and learn from users using Loras / Vector Stores All tools / APIs The following is a list of commands Carly can type into her discord chatbox to run commands. They have been edited to be more human readable.\nAuto Actions Web - Lets Carly spin up a headless docker where she can view a website Ask User - Lets Carly ask the person whom messaged her a question Ask LLM - Lets Carly ask Google Bard / ChatGPT a question Database Memory - Lets Carly recall past messages from all 4 databases Link API - Lets Carly spin up a headless docker to check out links then call \"Web Import\"API Based Actions Photo API - Lets Carly make raw photos Video API - Lets Carly make 4s videos (can take a few hours) IDE API - Lets Carly open and use a IDE in a docker Decktop API - Lets Carly use a full windowns or linux desktop in a dockerLora Actions Web Import - Lets Carly open a headless website and import the data into her ram Lora Importer - Imports a Lora into Carly's base model Lora Exporter - Exports a trained Lora to Luna's Hard Drive Lora web trainer - Takes web data imported by Carly, and trains a Lora model ontop of Carly's base modelOther Actions Autogen - Lets Carly start up a group chat with LLM models - https://github.com/microsoft/autogen Photo to Text API - Lets Carly see photos using a pretrained YOLOv8 model",
    "description": "",
    "tags": null,
    "title": "About Carly Kay",
    "uri": "/about-us/carly-api/index.html"
  },
  {
    "breadcrumb": "",
    "content": "How-tos These are the LocalAI How tos - Return to LocalAI\nThis section includes LocalAI end-to-end examples, tutorial and how-tos curated by the community and maintained by lunamidori5. To add your own How Tos, Please open a PR on this github - https://github.com/lunamidori5/Midori-AI\nSetup LocalAI with Docker Seting up a Model Making Text / LLM requests to LocalAI Making Photo / SD requests to LocalAI Programs and Demos This section includes other programs and how to setup, install, and use of LocalAI.\nMidori AI Subsystem Manager - lunamidori5 HA-OS Info - anto79_ops HA-OS x LocalAI - Maxi1134 Voice Assistance - Maxi1134 Thank you to our collaborators and volunteers TwinFinz: Help with the models template files and reviewing some code Crunchy: PR helping with both installers and removing 7zip need Maxi1134: Making our new HA-OS page for setting up LLM with HA ",
    "description": "",
    "tags": null,
    "title": "LocalAI How-tos",
    "uri": "/howtos/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "Curl Request Curl Chat API -\ncurl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{ \"model\": \"lunademo\", \"messages\": [{\"role\": \"user\", \"content\": \"How are you?\"}], \"temperature\": 0.9 }'Openai V1 - Recommended This is for Python, OpenAI=\u003eV1\nOpenAI Chat API Python -\nfrom openai import OpenAI client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"sk-xxx\") messages = [ {\"role\": \"system\", \"content\": \"You are LocalAI, a helpful, but really confused ai, you will only reply with confused emotes\"}, {\"role\": \"user\", \"content\": \"Hello How are you today LocalAI\"} ] completion = client.chat.completions.create( model=\"lunademo\", messages=messages, ) print(completion.choices[0].message)See OpenAI API for more info!\nOpenai V0 - Not Recommended This is for Python, OpenAI=0.28.1\nOpenAI Chat API Python -\nimport os import openai openai.api_base = \"http://localhost:8080/v1\" openai.api_key = \"sx-xxx\" OPENAI_API_KEY = \"sx-xxx\" os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY completion = openai.ChatCompletion.create( model=\"lunademo\", messages=[ {\"role\": \"system\", \"content\": \"You are LocalAI, a helpful, but really confused ai, you will only reply with confused emotes\"}, {\"role\": \"user\", \"content\": \"How are you?\"} ] ) print(completion.choices[0].message.content)OpenAI Completion API Python -\nimport os import openai openai.api_base = \"http://localhost:8080/v1\" openai.api_key = \"sx-xxx\" OPENAI_API_KEY = \"sx-xxx\" os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY completion = openai.Completion.create( model=\"lunademo\", prompt=\"function downloadFile(string url, string outputPath) \", max_tokens=256, temperature=0.5) print(completion.choices[0].text)",
    "description": "",
    "tags": null,
    "title": "Easy Request - All",
    "uri": "/howtos/by_hand/easy-request/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": " Home Assistant is an open-source home automation platform that allows users to control and monitor various smart devices in their homes. It supports a wide range of devices, including lights, thermostats, security systems, and more. The platform is designed to be user-friendly and customizable, enabling users to create automations and routines to make their homes more convenient and efficient. Home Assistant can be accessed through a web interface or a mobile app, and it can be installed on a variety of hardware platforms, such as Raspberry Pi or a dedicated server.\nCurrently, Home Assistant supports conversation-based agents and services. As of writing this, OpenAIs API is supported as a conversation agent; however, access to your homes devices and entities is possible through custom components. Local based services, such as LocalAI, are also available as a drop-in replacement for OpenAI services.\nIn this guide I will detail the steps I’ve taken to get Home-LLM and Local-AI working together in conjunction with Home-Assistant!\nThis guide assumes that you already have Local-AI running (in or out of the subsystem). If that is not done, you can Follow this How To or Install Using Midori AI Subsystem!\n1: You will first need to follow this guide to install Home-LLMinto your Home-Assistant installation.\nIf you simply want to install the Home-LLM component through HACS, you can press on this button:\nOpen your Home Assistant instance and open a repository inside the Home Assistant Community Store.\n2: Add Home LLM Conversation integration to HA.\n1: Access the Settings page. 2: Click on Devices \u0026 services. 3: Click on + ADD INTEGRATION on the lower-right part of the screen. 4: Type and then select Local LLM Conversation. 5: Select the Generic OpenAI Compatible API. 6: Enter the hostname or IP Address of your LocalAI host. 7: Enter the used port (Default is 8080 / 38080). 8: Enter mistral-7b-instruct-v0.3 as the Model Name* Leave API Key empty Do not check Use HTTPS leave API Path* as /v1 9: Press Next 10: Select Assist under Selected LLM API 11: Make sure the Prompt Format* is set to Mistral 12: Make sure Enable in context learning (ICL) examples is checked. 13: Press Sumbit 14: Press Finish 3: Configure the Voice assistant.\n1: Access the Settings page. 2: Click on Voice assistants. 3: Click on + ADD ASSISTANT. 4: Name the Assistant HomeLLM. 5: Select English as the Language. 6: Set the Conversation agent to the newly created LLM Model 'mistral-7b-instruct-v0.3' (remote). 7: Set your Speech-to-text Wake word, and Text-to-speech to the ones you use. Leave to None if you don’t have any. 8: Click Create 4: Select the newly created voice assistant as the default one.\nWhile remaining on the Voice assistants page click on the newly create assistant, and press the start at the top-right corner. There you go! Your Assistant should now be working with Local-AI through Home-LLM!\nMake sure that the entities you want to control are exposted to Assist within Home-Assistant! Notice Important Note:\nAny devices you choose to expose to the model will be added to the context and may have their state changed by the model. Only expose devices that you are comfortable with the model modifying, even if the modification is not what you intended. The model may occasionally hallucinate and issue commands to the wrong device. Use at your own risk.\n",
    "description": "",
    "tags": null,
    "title": "HA-OS (HomeLLM) x LocalAI",
    "uri": "/howtos/homellmxlocalai/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "Home Assistant is an open-source home automation platform that allows users to control and monitor various smart devices in their homes. It supports a wide range of devices, including lights, thermostats, security systems, and more. The platform is designed to be user-friendly and customizable, enabling users to create automations and routines to make their homes more convenient and efficient. Home Assistant can be accessed through a web interface or a mobile app, and it can be installed on a variety of hardware platforms, such as Raspberry Pi or a dedicated server.\nCurrently, Home Assistant supports conversation-based agents and services. As of writing this, OpenAIs API is supported as a conversation agent; however, access to your homes devices and entities is possible through custom components. Local based services, such as LocalAI, are also available as a drop-in replacement for OpenAI services.\nThere are multiple custom integrations available: Please note that both of the projects are similar in term of visual interfaces, they seem to be derived from the official Home Assistant plugin: OpenAI Conversation (to be confirmed)\nHome-LLM is a Home Assistant integration developed by Alex O’Connell (acon96) that allows for a completely local Large Language Model acting as a personal assistant. Using LocalAI as the backend is one of the supported platforms. The provided Large Language Models are specifically trained for home assistant and are therefore smaller in size. Extended OpenAI Conversation uses OpenAI API’s feature of function calling to call service of Home Assistant. Is more generic and work with most of the Large Language Model. Home-LLM Installation Instructions – LocalAI To install LocalAI, use our Midori AI Subsystem Manager\nInstallation Instructions – Home LLM (The HA plugin) Please follow the installation instructions on Home-LLM repo to install HACS plug-in.\nSetting up the plugin for HA \u0026 LocalAI Before adding the Llama Conversation agent in Home Assistant, you must download a LLM in the LocalAI models directory. Although you may use any model you want, this specific integration uses a model that has been specifically fine-tuned to work with Home Assistant. Performance will vary widely with other models.\nThe models can be found on the Midori AI model repo, as a part of the LocalAI manager.\nUse the Midori AI Subsystem Manager for a easy time installing models or follow Seting up a Model\nSetting up the “remote” backend: You will need the following settings in order to configure LocalAI backend:\nHostname: the host of the machine where LocalAI is installed and hosted. Port: The port you listed in your docker-compose.yaml (normally 8080) Name of the Model as exactly in the model.yaml file: This name must EXACTLY match the name as it appears in the file. The component will validate that the selected model is available for use and will ensure it is loaded remotely.\nOnce you have this information, proceed to “add Integration” in Home Assistant and search for “Llama Conversation” Here you will be greeted with a config flow to add the above information. Once the information is accepted, search your integrations for “Llama Conversation” and you can now view your settings including prompt, temperature, top K and other parameters. For LocalAI use, please make sure to select that ChatML prompt and to use ‘Use chat completions endpoint’.\nConfiguring the component as a Conversation Agent In order to utilize the conversation agent in HomeAssistant, you will need to configure it as a conversation agent. This can be done by following the the instructions here.\nChanging the prompt Example on how to use the prompt can be seen here.\nExtended OpenAI Conversation The project has been introduced here, and the Documentation is available directly on the author github project\nSetup summary LocalAI must be working with an installed LLM. You can directly ask the model if he is compatible with Home Assistant. To be confirmed: the model may work evene if it says he is not compatible. Mistral and Mixtral are compatible. Then install the Home Assistant integration, and follow the documentation provided above. High level Overview of the setup:\nadd the repository in HACS. install the integration. fill the needed information. You must fill something in the API key (if you don’t use api key just check the box “ignore authentication”), put the full url e.g. https://myLocalAIHostHere:8080/v1 (including /v1), Not sure: let the API version empty. configure the Home Assistant Assist using the new conversation agent. Notice Important Note:\nAny devices you choose to expose to the model will be added to the context and may have their state changed by the model. Only expose devices that you are comfortable with the model modifying, even if the modification is not what you intended. The model may occasionally hallucinate and issue commands to the wrong device. Use at your own risk.\n",
    "description": "",
    "tags": null,
    "title": "Home Assistant x LocalAI",
    "uri": "/howtos/setup-with-ha/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Midori AI Self-Hosted Models Repository Thank you for your interest in contributing to the Midori AI Self-Hosted Models’ model card repository! We welcome contributions from the community to help us maintain a comprehensive and up-to-date collection of model cards for self-hosted models.\nHow to Contribute To contribute a model card, please follow these steps:\nFork the Midori AI Repository to your GitHub account. Create a new branch in your forked repository where you will make your changes. Add your model card to the models directory. Follow the structure of the existing model cards to ensure consistency. Commit your changes and push them to your forked repository. Open a pull request from your forked repository to the master branch of the Midori AI Self-Hosted Models’ Model Card Repository. In the pull request, provide a clear and concise description of the changes you have made. Model Card Template The model card template provides guidance on the information to include in your model card. It covers aspects such as:\nModel Name: The name of the model you are describing. Model Description: A brief overview of the model’s purpose, architecture, and key features. Intended Use: Specify the tasks or applications for which the model is designed. Training Data: Describe the dataset(s) used to train the model, including their size, composition, and any relevant characteristics. Limitations and Biases: Discuss any known limitations or potential biases in the model, as well as steps taken to mitigate them. Ethical Considerations: Address any ethical implications or considerations related to the model’s use, such as privacy concerns or potential for discrimination. Deployment Details: If the model is deployed, provide information about the deployment environment, serving infrastructure, and any specific considerations for real-world usage. Review Process Once you have submitted a pull request, it will be reviewed by the Midori AI team. We will evaluate the quality and completeness of your model card based on the provided template. If there are any issues or suggestions for improvement, we will provide feedback and work with you to address them.\nMerging the Pull Request After addressing any feedback received during the review process, your pull request will be merged into the main branch of the Midori AI Self-Hosted Models’ Model Card Repository. Your model card will then be published and made available to the community.\nConclusion By contributing to the Midori AI Self-Hosted Models’ Model Card Repository, you help us build a valuable resource for the community. Your contributions will help users understand and evaluate self-hosted models more effectively, ultimately leading to improved model selection and usage.\nThank you for your contribution! Together, we can foster a more open and informed ecosystem for self-hosted AI models.\nUnleashing the Future of AI, Together.\n",
    "description": "",
    "tags": null,
    "title": "Models Repository",
    "uri": "/models/index.html"
  },
  {
    "breadcrumb": "LocalAI How-tos",
    "content": "In this guide I will explain how I’ve setup my Local voice assistant and satellites! A few softwares will be used in this guide.\nHACS for easy installation of the other tools on Home Assistant.\nLocalAI for the backend of the LLM.\nHome-LLM to connect our LocalAI instance to Home-assistant.\nHA-Fallback-Conversation to allow HA to use both the baked-in intent as well as the LLM as a fallback if no intent is found.\nWillow for the ESP32 sattelites.\nStep 1) Installing LocalAI We will start by installing LocalAI on our machine learning host.\nI recommend using a good machine with access to a GPU with at least 12 GB of Vram. As Willow itself can takes up to 6gb of Vram with another 4-5GB for our LLM model. I recommend keeping those loaded in the machine at all time for speedy reaction times on our satellites.\nHere an example of the VRAM usage for Willow and LocalAI with the Llampa 8B model:\n+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 555.42.02 Driver Version: 555.42.02 CUDA Version: 12.5 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA GeForce RTX 3090 Off | 00000000:01:00.0 Off | N/A | | 0% 39C P8 16W / 370W | 10341MiB / 24576MiB | 0% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | 0 N/A N/A 2862 C /opt/conda/bin/python 3646MiB | | 0 N/A N/A 2922 C /usr/bin/python 2108MiB | | 0 N/A N/A 2724851 C .../backend-assets/grpc/llama-cpp-avx2 4568MiB | +-----------------------------------------------------------------------------------------+I’ve chosen the Docker-Compose method for my LocalAI installation, this allows for easy management and easier upgrades when new relases are available.\nThis allows us to quickly create a container running LocalAI on our machine.\nIn order to do so, stop by the how to on how to setup a docker compose for LocalAI\nSetup LocalAI with Docker Compose\nOnce that is done simply use docker compose up -d and your LocalAI instance should now be available at: http://(hostipadress):8080/\nStep 1.a) Downloading the LLM model Once LocalAI if installed, you should be able to browse to the “Models” tab, that redirects to http://{{host}}:8080/browse. There we will search for the mistral-7b-instruct-v0.3 model and install it.\nOnce that is done, make sure that the model is working by heading to the Chat tab and selecting the model mistral-7b-instruct-v0.3 and initiating a chat.\nStep 2) Installing Home-LLM 1: You will first need to install the Home-LLM integration to Home-Assistant\nThankfuly, there is a neat link to do that easely on their repo!\nOpen your Home Assistant instance and open a repository inside the Home Assistant Community Store.\n2: Restart Home Assistant\n3: You will then need to add the Home LLM Conversation integration to Home-Assistant in order to connect LocalAI to it.\n1: Access the Settings page. 2: Click on Devices \u0026 services. 3: Click on + ADD INTEGRATION on the lower-right part of the screen. 4: Type and then select Local LLM Conversation. 5: Select the Generic OpenAI Compatible API. 6: Enter the hostname or IP Address of your LocalAI host. 7: Enter the used port (Default is 8080). 8: Enter mistral-7b-instruct-v0.3 as the Model Name* Leave API Key empty Do not check Use HTTPS leave API Path* as /v1 9: Press Next 10: Select Assist under Selected LLM API 11: Make sure the Prompt Format* is set to Mistral 12: Make sure Enable in context learning (ICL) examples is checked. 13: Press Sumbit 14: Press Finish Step 3) Installing HA-Fallback-Conversation 1: Integrate Fallback Conversation to Home-Assistant\n1: Access the HACS page. 2: Search for Fallback 3: Click on fallback_conversation. 4: Click on Download and install the integration 5: Restart Home Assistant for the integration to be detected. 6: Access the Settings page. 7: Click on Devices \u0026 services. 8: Click on + ADD INTEGRATION on the lower-right part of the screen. 8: Search for Fallback 9: Click on Fallback Conversation Agent. 10 Set the debug level at Some Debug for now. 11: Click Sumbit 2: Configure the Voice assistant within Home-assistant to use the newly added model through the Fallback Conversation Agent.\n1: Access the Settings page. 2: Click on Devices \u0026 services. 3: Click on Fallback Conversation Agent. 4: Click on CONFIGURE. 5: Select Home assistnat as the Primary Conversation Agent. 6: Select LLM MODEL 'mistral-7b-instruct-v0.3'(remote) as the Falback conversation Agent. Step 4) Selecting the right agent in the Voice assistant settings. 1: Integrate Fallback Conversation to Home-Assistant 1: Access the Settings page. 2: Click on Voice assistants page. 3: Click on Add Assistant. 4: Set the fields as wanted except for Conversation Agent. 5: Select Fallback Conversation Agent as the Conversation agent. Step 5) Setting up Willow Voice assistant satellites. Since willow is a more complex Software, I will simply leave Their guide here. I do recommend deploying your own Willow Inference Server in order to remain completely local!\nOnce the Willow sattelites are connencted to Home Assistant, they should automatically use your default Voice Assistant. Be sure to set the one using the fallback system as your favorite/default one!\n",
    "description": "",
    "tags": null,
    "title": "Voice Assistant HA-OS",
    "uri": "/howtos/voice_assistance_guide/index.html"
  },
  {
    "breadcrumb": "About",
    "content": "Contact Midori AI Thank you for your interest in Midori AI! We’re always happy to hear from others. If you have any questions, comments, or suggestions, please don’t hesitate to reach out to us. We aim to respond to all inquiries within 8 hours or less.\nEmail You can also reach us by email at contact-us@midori-ai.xyz.\nSocial Media Follow us on social media for the latest news and updates:\nTwitter: @lunamidori5 Facebook: Luna Midori Discord: Midori AI / The Cookie Club Contact Us Today! We look forward to hearing from you soon. Please don’t hesitate to reach out to us with any questions or concerns.\n",
    "description": "",
    "tags": null,
    "title": "Contact Us",
    "uri": "/about-us/contact-us/index.html"
  },
  {
    "breadcrumb": "Models Repository",
    "content": "All models are highly recommened for newer users as they are super easy to use and use the CHAT templ files from Twinz\nModel Size Description Links 7b CPU Friendly, small, okay quality https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-GGUF 2x7b Normal sized, good quality Removed for the time being, the model was acting up 8x7b Big, great quality https://huggingface.co/TheBloke/dolphin-2.7-mixtral-8x7b-GGUF 70b Large, hard to run, significant quality https://huggingface.co/TheBloke/dolphin-2.2-70B-GGUF Quant Mode Description Q3 Smallest , significant quality loss - not recommended Q4 Medium, balanced quality Q5 Large, very low quality loss - recommended for most users Q6 Very large, extremely low quality loss Q8 Extremely large, extremely low quality loss, hard to use - not recommended None Extremely large, No quality loss, super hard to use - really not recommended The minimum RAM and VRAM requirements for each model size, as a rough estimate.\n7b: System RAM: 10 GB / VRAM: 2 GB 2x7b: System RAM: 25 GB / VRAM: 8 GB 8x7b: System RAM: 55 GB / VRAM: 28 GB 70b: System RAM: 105 GB / VRAM: AI Card or better ",
    "description": "",
    "tags": null,
    "title": "Recommended Models",
    "uri": "/models/onsite_models/index.html"
  },
  {
    "breadcrumb": "Models Repository",
    "content": "All of these models originate from outside of the Midori AI model repository, and are not subject to the vetting process of Midori AI, although they are compatible with the model installer.\nNote that some of these models may deviate from our conventional model formatting standards (Quantized/Non-Quantized), and will be served using a rounding-down approach. For instance, if you request a Q8 model and none is available, the Q6 model will be served instead, and so on.\n3b-homellm-v1: 3BV1 3b-homellm-v2: 3BV2 1b-homellm-v1: 1BV1 ",
    "description": "",
    "tags": null,
    "title": "Offsite Supported Models",
    "uri": "/models/offsite_models/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
